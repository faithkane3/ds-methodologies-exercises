{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Natural-Language-Processing\" data-toc-modified-id=\"Natural-Language-Processing-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Natural Language Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-Natural-Language-Processing?\" data-toc-modified-id=\"What-is-Natural-Language-Processing?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><strong><font color=\"red\">What is Natural Language Processing?</font></strong></a></span></li><li><span><a href=\"#So-What?\" data-toc-modified-id=\"So-What?-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><strong><font color=\"orange\">So What?</font></strong></a></span></li><li><span><a href=\"#Normalization-Examples:\" data-toc-modified-id=\"Normalization-Examples:-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Normalization Examples:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Lowercase-Using-df.col.str.lower()\" data-toc-modified-id=\"Lowercase-Using-df.col.str.lower()-1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span><strong>Lowercase Using <code>df.col.str.lower()</code></strong></a></span></li><li><span><a href=\"#Normalize-Unicode-Characters\" data-toc-modified-id=\"Normalize-Unicode-Characters-1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span><strong>Normalize Unicode Characters</strong></a></span></li><li><span><a href=\"#Remove-Special-Characters-Using-Regex\" data-toc-modified-id=\"Remove-Special-Characters-Using-Regex-1.3.3\"><span class=\"toc-item-num\">1.3.3&nbsp;&nbsp;</span><strong>Remove Special Characters Using Regex</strong></a></span></li></ul></li><li><span><a href=\"#Now-What?\" data-toc-modified-id=\"Now-What?-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><strong><font color=\"green\">Now What?</font></strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Basic-Clean-Function\" data-toc-modified-id=\"Basic-Clean-Function-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Basic Clean Function</a></span></li></ul></li><li><span><a href=\"#Tokenization-Examples:\" data-toc-modified-id=\"Tokenization-Examples:-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Tokenization Examples:</a></span><ul class=\"toc-item\"><li><span><a href=\"#Using-.split()\" data-toc-modified-id=\"Using-.split()-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span><strong>Using <code>.split()</code></strong></a></span></li><li><span><a href=\"#Using-Regex\" data-toc-modified-id=\"Using-Regex-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Using Regex</a></span></li><li><span><a href=\"#Using-NLTK-Tokenization\" data-toc-modified-id=\"Using-NLTK-Tokenization-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Using NLTK Tokenization</a></span></li><li><span><a href=\"#Tokenize-Function\" data-toc-modified-id=\"Tokenize-Function-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Tokenize Function</a></span></li><li><span><a href=\"#Using-NLTK-PorterStemmer\" data-toc-modified-id=\"Using-NLTK-PorterStemmer-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span><strong>Using NLTK PorterStemmer</strong></a></span></li><li><span><a href=\"#Stem-Function\" data-toc-modified-id=\"Stem-Function-1.5.6\"><span class=\"toc-item-num\">1.5.6&nbsp;&nbsp;</span>Stem Function</a></span></li><li><span><a href=\"#Using-NLTK-WordNetLemmatizer\" data-toc-modified-id=\"Using-NLTK-WordNetLemmatizer-1.5.7\"><span class=\"toc-item-num\">1.5.7&nbsp;&nbsp;</span>Using NLTK WordNetLemmatizer</a></span></li><li><span><a href=\"#Lemmatize-Function\" data-toc-modified-id=\"Lemmatize-Function-1.5.8\"><span class=\"toc-item-num\">1.5.8&nbsp;&nbsp;</span>Lemmatize Function</a></span></li><li><span><a href=\"#Using-NLTK-Stopwords\" data-toc-modified-id=\"Using-NLTK-Stopwords-1.5.9\"><span class=\"toc-item-num\">1.5.9&nbsp;&nbsp;</span>Using NLTK Stopwords</a></span></li><li><span><a href=\"#Stopwords-Function\" data-toc-modified-id=\"Stopwords-Function-1.5.10\"><span class=\"toc-item-num\">1.5.10&nbsp;&nbsp;</span>Stopwords Function</a></span></li><li><span><a href=\"#Prep-Article-Function\" data-toc-modified-id=\"Prep-Article-Function-1.5.11\"><span class=\"toc-item-num\">1.5.11&nbsp;&nbsp;</span>Prep Article Function</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from acquire_walkthrough import get_news_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing\n",
    "\n",
    "#### **<font color=red>What is Natural Language Processing?</font>**\n",
    "\n",
    "Natural Language Processing allows you to use techniques in Python libraries like NLTK (Natural Language Tool Kit) and Spacy to create machine-useable structure out of natural language text. In other words, you can manipulate natural language in such a way that renders it useful in machine learning. Machines can't read words, but they can recognize numbers, so we have to process the text we want to use in a way that retains the original meaning while representing the text with numbers.\n",
    "\n",
    "#### **<font color=orange>So What?</font>**\n",
    "\n",
    "We need to know some basic terminology to get started:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalization** - is when you perform a series of tasks like making all text lowercase, removing punctuation, expanding contractions, removing anything that's not an ASCII character, etc.\n",
    "\n",
    "#### Normalization Examples:\n",
    "\n",
    "##### **Lowercase Using `df.col.str.lower()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>3 employees at Hyundai's Chennai plant test po...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Hyundai Motor India on Sunday said its three e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>CEO of New Zealand media outlet Stuff buys the...</td>\n",
       "      <td>Dharna</td>\n",
       "      <td>Sinead Boucher, the CEO of New Zealand-based m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Lockdown extensions won't help, cases will con...</td>\n",
       "      <td>Anushka Dixit</td>\n",
       "      <td>Mahindra Group Chairman Anand Mahindra said th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>102-year-old car rental firm Hertz files for b...</td>\n",
       "      <td>Dharna</td>\n",
       "      <td>Global car rental company Hertz has filed for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Jeff Bezos, Zuckerberg highest-earning billion...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Amazon CEO Jeff Bezos and Facebook CEO Mark Zu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                              title  \\\n",
       "0  business  3 employees at Hyundai's Chennai plant test po...   \n",
       "1  business  CEO of New Zealand media outlet Stuff buys the...   \n",
       "2  business  Lockdown extensions won't help, cases will con...   \n",
       "3  business  102-year-old car rental firm Hertz files for b...   \n",
       "4  business  Jeff Bezos, Zuckerberg highest-earning billion...   \n",
       "\n",
       "           author                                            content  \n",
       "0  Pragya Swastik  Hyundai Motor India on Sunday said its three e...  \n",
       "1          Dharna  Sinead Boucher, the CEO of New Zealand-based m...  \n",
       "2   Anushka Dixit  Mahindra Group Chairman Anand Mahindra said th...  \n",
       "3          Dharna  Global car rental company Hertz has filed for ...  \n",
       "4  Pragya Swastik  Amazon CEO Jeff Bezos and Facebook CEO Mark Zu...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_news_articles()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note I have not reassigned this or changed the inplace argument to True yet; just a look.\n",
    "\n",
    "df.content.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Normalize Unicode Characters**\n",
    "\n",
    "[Here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.normalize.html) is the documentation for using `unicodedata.normalize()` on a Pandas Series.\n",
    "\n",
    "```python\n",
    "df.col.str.normalize(form, unistr)\n",
    "```\n",
    "\n",
    "```python\n",
    "df.col.str.encode('ascii', 'ignore')\n",
    "```\n",
    "\n",
    "```python\n",
    "df.col.str.decode('utf-8', 'ignore')\n",
    "```\n",
    "\n",
    "```python\n",
    "df.col.str.replace(r\"[^A-z0-9'\\s]\", '', regex=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is a look because it has not been reassigned or changed in place.\n",
    "\n",
    "df.content.str.normalize('NFKC').str.encode('ascii', 'ignore').str.decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Remove Special Characters Using Regex**\n",
    "\n",
    "I found [this article](https://kanoki.org/2019/11/12/how-to-use-regex-in-pandas/) very helpful when using Regex in Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, this is a look because it has not been reassigned or changed in place.\n",
    "\n",
    "df.content.str.replace(r\"[^A-z0-9'\\s]\", '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, I can chain these together and reassign to my df as a new columm\n",
    "\n",
    "df['basic_clean'] = df.content.str.lower()\\\n",
    "                    .str.replace(r\"[^A-z0-9'\\s]\", '', regex=True)\\\n",
    "                    .str.normalize('NFKC')\\\n",
    "                    .str.encode('ascii', 'ignore')\\\n",
    "                    .str.decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<font color=green>Now What?</font>**\n",
    "\n",
    "##### Basic Clean Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(df, col):\n",
    "    '''\n",
    "    This function takes in a df and a string for a column and\n",
    "    returns the df with a new column named 'basic_clean' with the\n",
    "    passed column text normalized.\n",
    "    '''\n",
    "    df['basic_clean'] = df[col].str.lower()\\\n",
    "                    .str.replace(r\"[^A-z0-9'\\s]\", '', regex=True)\\\n",
    "                    .str.normalize('NFKC')\\\n",
    "                    .str.encode('ascii', 'ignore')\\\n",
    "                    .str.decode('utf-8', 'ignore')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = basic_clean(df, 'content')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization** - is when you split larger strings of text into smaller pieces or tokens by setting a boundary. You might chunk a sentence into words using a space as a boundary or a paragraph into sentences using punctuation as a boundary.\n",
    "\n",
    "#### Tokenization Examples:\n",
    "\n",
    "##### **Using `.split()`**\n",
    "\n",
    "Tokenizing using `.split()` is simple but also limited to one delimiter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Knowledge is the compound interest of curiosity. - James Clear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"There's the kind of person who is always the victim in any story they tell. Always on the receiving end of some injustice. There's the kind of person who is always the kind of hero of every story they tell. There's the smart person; they delivered the clever put down there.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using Regex\n",
    "\n",
    "**<font color=purple>Identifiers</font>**\n",
    "\n",
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n",
    "\n",
    "<tr ><td><span >\\d</span></td><td>A digit</td><td>file_\\d\\d</td><td>file_25</td></tr>\n",
    "\n",
    "<tr ><td><span >\\w</span></td><td>Alphanumeric</td><td>\\w-\\w\\w\\w</td><td>A-b_1</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\s</span></td><td>White space</td><td>a\\sb\\sc</td><td>a b c</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >\\D</span></td><td>A non digit</td><td>\\D\\D\\D</td><td>ABC</td></tr>\n",
    "\n",
    "<tr ><td><span >\\W</span></td><td>Non-alphanumeric</td><td>\\W\\W\\W\\W\\W</td><td>*-+=)</td></tr>\n",
    "\n",
    "<tr ><td><span >\\S</span></td><td>Non-whitespace</td><td>\\S\\S\\S\\S</td><td>Yoyo</td></tr></table>\n",
    "\n",
    "**<font color=purple>Quantifiers</font>**\n",
    "\n",
    "<table ><tr><th>Character</th><th>Description</th><th>Example Pattern Code</th><th >Exammple Match</th></tr>\n",
    "\n",
    "<tr ><td><span >+</span></td><td>Occurs one or more times</td><td>\tVersion \\w-\\w+</td><td>Version A-b1_1</td></tr>\n",
    "\n",
    "<tr ><td><span >{3}</span></td><td>Occurs exactly 3 times</td><td>\\D{3}</td><td>abc</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{2,4}</span></td><td>Occurs 2 to 4 times</td><td>\\d{2,4}</td><td>123</td></tr>\n",
    "\n",
    "\n",
    "\n",
    "<tr ><td><span >{3,}</span></td><td>Occurs 3 or more</td><td>\\w{3,}</td><td>anycharacters</td></tr>\n",
    "\n",
    "<tr ><td><span >\\*</span></td><td>Occurs zero or more times</td><td>A\\*B\\*C*</td><td>AAACC</td></tr>\n",
    "\n",
    "<tr ><td><span >?</span></td><td>Once or none</td><td>plurals?</td><td>plural</td></tr></table>\n",
    "\n",
    "**<font color=purple>More Regex</font>**\n",
    "\n",
    "<table ><tr><th>Character</th><th>Description</th><th>Example</th></tr>\n",
    "    \n",
    "<tr ><td><span >|</span></td><td>or statement</td><td>r'dog|cat'</td></tr>\n",
    "\n",
    "<tr ><td><span >*</span></td><td>wildcard</td><td>r'.at'</td></tr>\n",
    "    \n",
    "<tr ><td><span >^</span></td><td>starts with</td><td>r'^\\d'</td></tr>\n",
    "    \n",
    "<tr ><td><span >[^]</span></td><td>exclusion</td><td>r'[^a-z]'</td></tr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split your text using a regex pattern in .findall()\n",
    "\n",
    "pattern = r'[\\w]+'\n",
    "text = 'Knowledge is the compound interest of curiosity. - James Clear'\n",
    "\n",
    "tokens = re.findall(pattern, text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `.compile()` with .split(text) to split your text on more than one delimiter\n",
    "\n",
    "pattern = re.compile(r'[.;!?]')\n",
    "text = \"\"\"There's the kind of person who is always the victim in any story they tell. Always on the receiving end of some injustice. There's the kind of person who is always the kind of hero of every story they tell. There's the smart person; they delivered the clever put down there.\"\"\"\n",
    "\n",
    "pattern.split(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using NLTK Tokenization\n",
    "\n",
    "```python\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "df.col.apply(tokenizer.tokenize).str.join(' ')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.tokenize.ToktokTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.basic_clean.apply(tokenizer.tokenize)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we apply nltk's tokenizer to each row, or text, in our basic_clean Series\n",
    "\n",
    "df['clean_tokes'] = df.basic_clean.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>Now What?</font>**\n",
    "\n",
    "##### Tokenize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, col):\n",
    "    '''\n",
    "    This function takes in a df and a string for a column and\n",
    "    returns a df with a new column named 'clean_tokes' with the\n",
    "    passed column text tokenized and in a list.\n",
    "    '''\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    df['clean_tokes'] = df[col].apply(tokenizer.tokenize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tokenize(df, 'basic_clean')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Using NLTK PorterStemmer**\n",
    "\n",
    "```python\n",
    "ps = nltk.porter.PorterStemmer()\n",
    "```\n",
    "\n",
    "```python\n",
    "stems = df.col.apply(lambda row: [ps.stem(word) for word in row])\n",
    "```\n",
    "\n",
    "```python\n",
    "df['stemmed'] = stems.str.join(' ')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.stem('turning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series of lists of stemmed words using our cleaned tokens\n",
    "\n",
    "stems = df.clean_tokes.apply(lambda row: [ps.stem(word) for word in row])\n",
    "stems.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join our cleaned, stemmed lists of words back into strings of words/tokens\n",
    "\n",
    "df['stemmed'] = stems.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 300\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>Now What?</font>**\n",
    "\n",
    "##### Stem Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(df, col):\n",
    "    '''\n",
    "    This function takes in a df and a string for a column name and\n",
    "    returns a df with a new column named 'stemmed'.\n",
    "    '''\n",
    "    # Create porter stemmer\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Create a Series of lists of stemmed words using our cleaned tokens\n",
    "    stems = df[col].apply(lambda row: [ps.stem(word) for word in row])\n",
    "    \n",
    "    # Join our cleaned, stemmed lists of words back into strings of words/tokens\n",
    "    df['stemmed'] = stems.str.join(' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = stem(df, 'clean_tokes')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using NLTK WordNetLemmatizer\n",
    "\n",
    "```python\n",
    "nltk.download('wordnet')\n",
    "```\n",
    "\n",
    "```python\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "```\n",
    "\n",
    "```python\n",
    "lemmas = df.col.apply(lambda row: [wnl.lemmatize(word) for word in row])\n",
    "```\n",
    "\n",
    "```python\n",
    "df['lemmatized'] = lemmas.str.join(' ')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = df.clean_tokes.apply(lambda row: [wnl.lemmatize(word) for word in row])\n",
    "lemmas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join our cleaned, lemmatized lists of words back into strings of words/tokens\n",
    "\n",
    "df['lemmatized'] = lemmas.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>Now What?</font>**\n",
    "\n",
    "##### Lemmatize Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(df, col):\n",
    "    '''\n",
    "    This function takes in a df and a string for column name and\n",
    "    returns a the original df with a new column called 'lemmatized'.\n",
    "    '''\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = df[col].apply(lambda row: [wnl.lemmatize(word) for word in row])\n",
    "    df['lemmatized'] = lemmas.str.join(' ')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lemmatize(df, 'clean_tokes')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using NLTK Stopwords\n",
    "\n",
    "```python\n",
    "import nltk; nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "```python\n",
    "stopword_list = stopwords.words('english')\n",
    "```\n",
    "\n",
    "```python\n",
    "words = df.col.str.split()\n",
    "```\n",
    "\n",
    "```python\n",
    "filtered_words = words.apply(lambda row: [word for word in row if word not in stopword_list])\n",
    "```\n",
    "\n",
    "```python\n",
    "df['clean_words'] = filtered_words.str.join(' ')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split words in column\n",
    "words = df.lemmatized.str.split()\n",
    "\n",
    "# Check each words in each row of the column against stopword_list and return only those that are not\n",
    "filtered_words = words.apply(lambda row: [word for word in row if word not in stopword_list])\n",
    "df['clean_lemmas'] = filtered_words.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = df.stemmed.str.split()\n",
    "filtered_words = words.apply(lambda row: [word for word in row if word not in stopword_list])\n",
    "df['clean_stemmed'] = filtered_words.str.join(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>Now What?</font>**\n",
    "\n",
    "##### Stopwords Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(df, col):\n",
    "    '''\n",
    "    This function takes in a df and a string for column name and \n",
    "    returns the df with a new column named 'clean' with stopwords removed.\n",
    "    '''\n",
    "    # Create stopword_list\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Split words in column\n",
    "    words = df[col].str.split()\n",
    "    \n",
    "    # Check each words in each row of the column against stopword_list and return only those that are not\n",
    "    filtered_words = words.apply(lambda row: [word for word in row if word not in stopword_list])\n",
    "    \n",
    "    # Create new column of words that have stopwords removed\n",
    "    df['clean_' + col] = filtered_words.str.join(' ')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = remove_stopwords(df, 'lemmatized')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=green>Now What?</font>**\n",
    "\n",
    "##### Prep Article Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['topic', 'title', 'author', 'content'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df):\n",
    "    '''\n",
    "    This function takes in the news articles df and\n",
    "    returns the df with original columns plus cleaned\n",
    "    and lemmatized content without stopwords.\n",
    "    '''\n",
    "    # Do basic clean on article content\n",
    "    df = basic_clean(df, 'content')\n",
    "    \n",
    "    # Tokenize clean article content\n",
    "    df = tokenize(df, 'basic_clean')\n",
    "    \n",
    "    # Stem cleaned and tokenized article content\n",
    "    df = stem(df, 'clean_tokes')\n",
    "    \n",
    "    # Remove stopwords from Lemmatized article content\n",
    "    df = remove_stopwords(df, 'stemmed')\n",
    "    \n",
    "    # Lemmatize cleaned and tokenized article content\n",
    "    df = lemmatize(df, 'clean_tokes')\n",
    "    \n",
    "    # Remove stopwords from Lemmatized article content\n",
    "    df = remove_stopwords(df, 'lemmatized')\n",
    "    \n",
    "    return df[['topic', 'title', 'author', 'content', 'clean_stemmed', 'clean_lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_stemmed</th>\n",
       "      <th>clean_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>3 employees at Hyundai's Chennai plant test po...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Hyundai Motor India on Sunday said its three e...</td>\n",
       "      <td>hyundai motor india sunday said three employe ...</td>\n",
       "      <td>hyundai motor india sunday said three employee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>CEO of New Zealand media outlet Stuff buys the...</td>\n",
       "      <td>Dharna</td>\n",
       "      <td>Sinead Boucher, the CEO of New Zealand-based m...</td>\n",
       "      <td>sinead boucher ceo new zealandbas media outlet...</td>\n",
       "      <td>sinead boucher ceo new zealandbased medium out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Lockdown extensions won't help, cases will con...</td>\n",
       "      <td>Anushka Dixit</td>\n",
       "      <td>Mahindra Group Chairman Anand Mahindra said th...</td>\n",
       "      <td>mahindra group chairman anand mahindra said lo...</td>\n",
       "      <td>mahindra group chairman anand mahindra said lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>102-year-old car rental firm Hertz files for b...</td>\n",
       "      <td>Dharna</td>\n",
       "      <td>Global car rental company Hertz has filed for ...</td>\n",
       "      <td>global car rental compani hertz ha file bankru...</td>\n",
       "      <td>global car rental company hertz ha filed bankr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Jeff Bezos, Zuckerberg highest-earning billion...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Amazon CEO Jeff Bezos and Facebook CEO Mark Zu...</td>\n",
       "      <td>amazon ceo jeff bezo facebook ceo mark zuckerb...</td>\n",
       "      <td>amazon ceo jeff bezos facebook ceo mark zucker...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                              title  \\\n",
       "0  business  3 employees at Hyundai's Chennai plant test po...   \n",
       "1  business  CEO of New Zealand media outlet Stuff buys the...   \n",
       "2  business  Lockdown extensions won't help, cases will con...   \n",
       "3  business  102-year-old car rental firm Hertz files for b...   \n",
       "4  business  Jeff Bezos, Zuckerberg highest-earning billion...   \n",
       "\n",
       "           author                                            content  \\\n",
       "0  Pragya Swastik  Hyundai Motor India on Sunday said its three e...   \n",
       "1          Dharna  Sinead Boucher, the CEO of New Zealand-based m...   \n",
       "2   Anushka Dixit  Mahindra Group Chairman Anand Mahindra said th...   \n",
       "3          Dharna  Global car rental company Hertz has filed for ...   \n",
       "4  Pragya Swastik  Amazon CEO Jeff Bezos and Facebook CEO Mark Zu...   \n",
       "\n",
       "                                       clean_stemmed  \\\n",
       "0  hyundai motor india sunday said three employe ...   \n",
       "1  sinead boucher ceo new zealandbas media outlet...   \n",
       "2  mahindra group chairman anand mahindra said lo...   \n",
       "3  global car rental compani hertz ha file bankru...   \n",
       "4  amazon ceo jeff bezo facebook ceo mark zuckerb...   \n",
       "\n",
       "                                    clean_lemmatized  \n",
       "0  hyundai motor india sunday said three employee...  \n",
       "1  sinead boucher ceo new zealandbased medium out...  \n",
       "2  mahindra group chairman anand mahindra said lo...  \n",
       "3  global car rental company hertz ha filed bankr...  \n",
       "4  amazon ceo jeff bezos facebook ceo mark zucker...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prep_article_data(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
