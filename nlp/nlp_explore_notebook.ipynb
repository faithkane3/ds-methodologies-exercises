{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#What-is-Text-Data?\" data-toc-modified-id=\"What-is-Text-Data?-0.0.1\"><span class=\"toc-item-num\">0.0.1&nbsp;&nbsp;</span><strong><font color=\"red\">What is Text Data?</font></strong></a></span></li></ul></li><li><span><a href=\"#Important-Terminology\" data-toc-modified-id=\"Important-Terminology-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span><strong>Important Terminology</strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#So-What-Tools-and-Techniques-Can-I-use-to-Explore-Text-Data?\" data-toc-modified-id=\"So-What-Tools-and-Techniques-Can-I-use-to-Explore-Text-Data?-0.1.1\"><span class=\"toc-item-num\">0.1.1&nbsp;&nbsp;</span><strong><font color=\"orange\">So What Tools and Techniques Can I use to Explore Text Data?</font></strong></a></span></li><li><span><a href=\"#Now-What?\" data-toc-modified-id=\"Now-What?-0.1.2\"><span class=\"toc-item-num\">0.1.2&nbsp;&nbsp;</span><strong><font color=\"green\">Now What?</font></strong></a></span></li></ul></li></ul></li><li><span><a href=\"#Acquire-News-Articles\" data-toc-modified-id=\"Acquire-News-Articles-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Acquire News Articles</a></span></li><li><span><a href=\"#Prepare-News-Articles\" data-toc-modified-id=\"Prepare-News-Articles-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare News Articles</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extra-Cleaning\" data-toc-modified-id=\"Extra-Cleaning-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Extra Cleaning</a></span></li></ul></li><li><span><a href=\"#Explore-Label-Column\" data-toc-modified-id=\"Explore-Label-Column-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Explore Label Column</a></span></li><li><span><a href=\"#Create-Numeric-Columns-from-Text\" data-toc-modified-id=\"Create-Numeric-Columns-from-Text-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create Numeric Columns from Text</a></span></li><li><span><a href=\"#Identify-Important-Words-in-Text\" data-toc-modified-id=\"Identify-Important-Words-in-Text-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Identify Important Words in Text</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-and-Visualize-N-grams\" data-toc-modified-id=\"Create-and-Visualize-N-grams-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Create and Visualize N-grams</a></span><ul class=\"toc-item\"><li><span><a href=\"#Create-N-grams-Function\" data-toc-modified-id=\"Create-N-grams-Function-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Create N-grams Function</a></span></li><li><span><a href=\"#Create-N-Gram-Viz-Functions\" data-toc-modified-id=\"Create-N-Gram-Viz-Functions-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Create N-Gram Viz Functions</a></span></li></ul></li><li><span><a href=\"#Explore-Words-by-Topic-Label\" data-toc-modified-id=\"Explore-Words-by-Topic-Label-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Explore Words by Topic Label</a></span><ul class=\"toc-item\"><li><span><a href=\"#Explore-Business\" data-toc-modified-id=\"Explore-Business-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;</span>Explore Business</a></span></li><li><span><a href=\"#Explore-Entertainment\" data-toc-modified-id=\"Explore-Entertainment-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;</span>Explore Entertainment</a></span></li><li><span><a href=\"#Explore-Sports\" data-toc-modified-id=\"Explore-Sports-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;</span>Explore Sports</a></span></li><li><span><a href=\"#Explore-Tech\" data-toc-modified-id=\"Explore-Tech-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;</span>Explore Tech</a></span></li></ul></li><li><span><a href=\"#Create-a-Series-of-Word-Frequencies-for-Each-Topic-Label\" data-toc-modified-id=\"Create-a-Series-of-Word-Frequencies-for-Each-Topic-Label-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Create a Series of Word Frequencies for Each Topic Label</a></span></li><li><span><a href=\"#Create-df-of-Word-Frequencies-for-Each-Subset-Above\" data-toc-modified-id=\"Create-df-of-Word-Frequencies-for-Each-Subset-Above-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Create df of Word Frequencies for Each Subset Above</a></span></li><li><span><a href=\"#Search-for-Topic-specific-Words\" data-toc-modified-id=\"Search-for-Topic-specific-Words-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Search for Topic-specific Words</a></span></li><li><span><a href=\"#Word-Clouds!\" data-toc-modified-id=\"Word-Clouds!-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Word Clouds!</a></span><ul class=\"toc-item\"><li><span><a href=\"#Standard-Word-Cloud\" data-toc-modified-id=\"Standard-Word-Cloud-5.6.1\"><span class=\"toc-item-num\">5.6.1&nbsp;&nbsp;</span>Standard Word Cloud</a></span></li><li><span><a href=\"#Customized-Word-Cloud\" data-toc-modified-id=\"Customized-Word-Cloud-5.6.2\"><span class=\"toc-item-num\">5.6.2&nbsp;&nbsp;</span>Customized Word Cloud</a></span></li><li><span><a href=\"#Bigram-Word-Cloud\" data-toc-modified-id=\"Bigram-Word-Cloud-5.6.3\"><span class=\"toc-item-num\">5.6.3&nbsp;&nbsp;</span>Bigram Word Cloud</a></span></li><li><span><a href=\"#Shaped-Word-Cloud\" data-toc-modified-id=\"Shaped-Word-Cloud-5.6.4\"><span class=\"toc-item-num\">5.6.4&nbsp;&nbsp;</span>Shaped Word Cloud</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "from pylab import *\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from acquire_walkthrough import get_news_articles\n",
    "from prepare_walkthrough import prep_article_data, remove_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Text Data\n",
    "\n",
    "#### **<font color=red>What is Text Data?</font>**\n",
    "\n",
    "### **Important Terminology**\n",
    "\n",
    "**Corpus -** is the entire body of text. A collection of articles is a corpus; the text from one article is a document.\n",
    "\n",
    "**Tokens -** are chunks of data; they could be words, punctuation, sentences, numbers, etc. You must decide what tokens will be most useful to you for your specific goals. \n",
    "\n",
    "**N-grams -** are token groups of n words that appear together in a document or corpus. You can examine uni-, bi-, and trigrams as a way to gain context for your tokens and find important word combinations.\n",
    "\n",
    "**Stop Words -** are words that are either so common throughout my corpus or so devoid of meaning that they become noise as I explore my text data. Removing these can help the real story emerge from meaningful words in your text.\n",
    "\n",
    "#### **<font color=orange>So What Tools and Techniques Can I use to Explore Text Data?</font>**\n",
    "\n",
    "[Word Clouds](#word_clouds) - are a good way to visualize word frequencies and word and phrase importance in your text data. \n",
    "\n",
    "[N-grams](#n_grams) - can be examined and visualized by returning the ngrams generated from a sequence of items, as an iterator.\n",
    "\n",
    "```python\n",
    "ngrams(sequence, n)\n",
    "```\n",
    "\n",
    "#### **<font color=green>Now What?</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire News Articles\n",
    "\n",
    "Here I pull in the text data I scraped used BeautifulSoup in [this review](https://faithkane3.github.io/beautiful_soup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_news_articles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare News Articles\n",
    "\n",
    "I'm going to use my cleaned, lemmatized text with basic stopwords removed to start; that is how my `prep_article_data` from [this review](https://faithkane3.github.io/nlp_review_notebook) will deliver my text to me here. As I explore, I will probably be removing more stopwords and tokens that are just noise for my puposes as I come across them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prep_article_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_stemmed</th>\n",
       "      <th>clean_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>US firm buys Serum Institute parent's Czech un...</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>US biotech firm Novavax has announced it's buy...</td>\n",
       "      <td>us biotech firm novavax ha announc buy czech r...</td>\n",
       "      <td>u biotech firm novavax ha announced buying cze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Twitter CEO donates $10M to project giving $1,...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Twitter's billionaire CEO Jack Dorsey has dona...</td>\n",
       "      <td>twitter billionair ceo jack dorsey ha donat 10...</td>\n",
       "      <td>twitter billionaire ceo jack dorsey ha donated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Google in talks to buy 5% stake in Vodafone Id...</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>Google is exploring an investment in Vodafone ...</td>\n",
       "      <td>googl explor invest vodafon idea part us techn...</td>\n",
       "      <td>google exploring investment vodafone idea part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Microsoft in talks to buy 2.5% stake in Jio fo...</td>\n",
       "      <td>Anushka Dixit</td>\n",
       "      <td>Microsoft is in talks with Mukesh Ambani-led R...</td>\n",
       "      <td>microsoft talk mukesh ambanil relianc industri...</td>\n",
       "      <td>microsoft talk mukesh ambaniled reliance indus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>25-year-old Anant Ambani joins $65 billion Jio...</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>Asia's richest person Mukesh Ambani's 25-year-...</td>\n",
       "      <td>asia richest person mukesh ambani 25yearold so...</td>\n",
       "      <td>asia richest person mukesh ambanis 25yearold s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                              title  \\\n",
       "0  business  US firm buys Serum Institute parent's Czech un...   \n",
       "1  business  Twitter CEO donates $10M to project giving $1,...   \n",
       "2  business  Google in talks to buy 5% stake in Vodafone Id...   \n",
       "3  business  Microsoft in talks to buy 2.5% stake in Jio fo...   \n",
       "4  business  25-year-old Anant Ambani joins $65 billion Jio...   \n",
       "\n",
       "                   author                                            content  \\\n",
       "0  Krishna Veera Vanamali  US biotech firm Novavax has announced it's buy...   \n",
       "1          Pragya Swastik  Twitter's billionaire CEO Jack Dorsey has dona...   \n",
       "2  Krishna Veera Vanamali  Google is exploring an investment in Vodafone ...   \n",
       "3           Anushka Dixit  Microsoft is in talks with Mukesh Ambani-led R...   \n",
       "4  Krishna Veera Vanamali  Asia's richest person Mukesh Ambani's 25-year-...   \n",
       "\n",
       "                                       clean_stemmed  \\\n",
       "0  us biotech firm novavax ha announc buy czech r...   \n",
       "1  twitter billionair ceo jack dorsey ha donat 10...   \n",
       "2  googl explor invest vodafon idea part us techn...   \n",
       "3  microsoft talk mukesh ambanil relianc industri...   \n",
       "4  asia richest person mukesh ambani 25yearold so...   \n",
       "\n",
       "                                    clean_lemmatized  \n",
       "0  u biotech firm novavax ha announced buying cze...  \n",
       "1  twitter billionaire ceo jack dorsey ha donated...  \n",
       "2  google exploring investment vodafone idea part...  \n",
       "3  microsoft talk mukesh ambaniled reliance indus...  \n",
       "4  asia richest person mukesh ambanis 25yearold s...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cleaning'></a>\n",
    "### Extra Cleaning\n",
    "\n",
    "As I explore my text data, I will find more things I want to remove. I know from exploring the text I scraped from inshort articles that there are a lot of numbers that are not useful to me for my purposes of sentiment analysis and creating a machine learning model that can predict the topic, or label, of an article. I'm going to remove numbers from my text here, and as I find more unnecessary tokens, I will return here to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_stemmed</th>\n",
       "      <th>clean_lemmatized</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>US firm buys Serum Institute parent's Czech un...</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>US biotech firm Novavax has announced it's buy...</td>\n",
       "      <td>us biotech firm novavax ha announc buy czech r...</td>\n",
       "      <td>u biotech firm novavax ha announced buying cze...</td>\n",
       "      <td>u biotech firm novavax ha announced buying cze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Twitter CEO donates $10M to project giving $1,...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Twitter's billionaire CEO Jack Dorsey has dona...</td>\n",
       "      <td>twitter billionair ceo jack dorsey ha donat 10...</td>\n",
       "      <td>twitter billionaire ceo jack dorsey ha donated...</td>\n",
       "      <td>twitter billionaire ceo jack dorsey ha donated...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                              title  \\\n",
       "0  business  US firm buys Serum Institute parent's Czech un...   \n",
       "1  business  Twitter CEO donates $10M to project giving $1,...   \n",
       "\n",
       "                   author                                            content  \\\n",
       "0  Krishna Veera Vanamali  US biotech firm Novavax has announced it's buy...   \n",
       "1          Pragya Swastik  Twitter's billionaire CEO Jack Dorsey has dona...   \n",
       "\n",
       "                                       clean_stemmed  \\\n",
       "0  us biotech firm novavax ha announc buy czech r...   \n",
       "1  twitter billionair ceo jack dorsey ha donat 10...   \n",
       "\n",
       "                                    clean_lemmatized  \\\n",
       "0  u biotech firm novavax ha announced buying cze...   \n",
       "1  twitter billionaire ceo jack dorsey ha donated...   \n",
       "\n",
       "                                                text  \n",
       "0  u biotech firm novavax ha announced buying cze...  \n",
       "1  twitter billionaire ceo jack dorsey ha donated...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace numbers in my text column with empty strings.\n",
    "\n",
    "df['text'] = df.clean_lemmatized.replace(r'\\d', '', regex=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>clean_stemmed</th>\n",
       "      <th>clean_lemmatized</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>US firm buys Serum Institute parent's Czech un...</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>US biotech firm Novavax has announced it's buy...</td>\n",
       "      <td>us biotech firm novavax ha announc buy czech r...</td>\n",
       "      <td>u biotech firm novavax ha announced buying cze...</td>\n",
       "      <td>u biotech firm novavax ha announced buying cze...</td>\n",
       "      <td>biotech firm novavax announced buying czech re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Twitter CEO donates $10M to project giving $1,...</td>\n",
       "      <td>Pragya Swastik</td>\n",
       "      <td>Twitter's billionaire CEO Jack Dorsey has dona...</td>\n",
       "      <td>twitter billionair ceo jack dorsey ha donat 10...</td>\n",
       "      <td>twitter billionaire ceo jack dorsey ha donated...</td>\n",
       "      <td>twitter billionaire ceo jack dorsey ha donated...</td>\n",
       "      <td>twitter billionaire ceo jack dorsey donated mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Google in talks to buy 5% stake in Vodafone Id...</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>Google is exploring an investment in Vodafone ...</td>\n",
       "      <td>googl explor invest vodafon idea part us techn...</td>\n",
       "      <td>google exploring investment vodafone idea part...</td>\n",
       "      <td>google exploring investment vodafone idea part...</td>\n",
       "      <td>google exploring investment vodafone idea part...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>Microsoft in talks to buy 2.5% stake in Jio fo...</td>\n",
       "      <td>Anushka Dixit</td>\n",
       "      <td>Microsoft is in talks with Mukesh Ambani-led R...</td>\n",
       "      <td>microsoft talk mukesh ambanil relianc industri...</td>\n",
       "      <td>microsoft talk mukesh ambaniled reliance indus...</td>\n",
       "      <td>microsoft talk mukesh ambaniled reliance indus...</td>\n",
       "      <td>microsoft talk mukesh ambaniled reliance indus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>25-year-old Anant Ambani joins $65 billion Jio...</td>\n",
       "      <td>Krishna Veera Vanamali</td>\n",
       "      <td>Asia's richest person Mukesh Ambani's 25-year-...</td>\n",
       "      <td>asia richest person mukesh ambani 25yearold so...</td>\n",
       "      <td>asia richest person mukesh ambanis 25yearold s...</td>\n",
       "      <td>asia richest person mukesh ambanis yearold son...</td>\n",
       "      <td>asia richest person mukesh ambanis yearold son...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic                                              title  \\\n",
       "0  business  US firm buys Serum Institute parent's Czech un...   \n",
       "1  business  Twitter CEO donates $10M to project giving $1,...   \n",
       "2  business  Google in talks to buy 5% stake in Vodafone Id...   \n",
       "3  business  Microsoft in talks to buy 2.5% stake in Jio fo...   \n",
       "4  business  25-year-old Anant Ambani joins $65 billion Jio...   \n",
       "\n",
       "                   author                                            content  \\\n",
       "0  Krishna Veera Vanamali  US biotech firm Novavax has announced it's buy...   \n",
       "1          Pragya Swastik  Twitter's billionaire CEO Jack Dorsey has dona...   \n",
       "2  Krishna Veera Vanamali  Google is exploring an investment in Vodafone ...   \n",
       "3           Anushka Dixit  Microsoft is in talks with Mukesh Ambani-led R...   \n",
       "4  Krishna Veera Vanamali  Asia's richest person Mukesh Ambani's 25-year-...   \n",
       "\n",
       "                                       clean_stemmed  \\\n",
       "0  us biotech firm novavax ha announc buy czech r...   \n",
       "1  twitter billionair ceo jack dorsey ha donat 10...   \n",
       "2  googl explor invest vodafon idea part us techn...   \n",
       "3  microsoft talk mukesh ambanil relianc industri...   \n",
       "4  asia richest person mukesh ambani 25yearold so...   \n",
       "\n",
       "                                    clean_lemmatized  \\\n",
       "0  u biotech firm novavax ha announced buying cze...   \n",
       "1  twitter billionaire ceo jack dorsey ha donated...   \n",
       "2  google exploring investment vodafone idea part...   \n",
       "3  microsoft talk mukesh ambaniled reliance indus...   \n",
       "4  asia richest person mukesh ambanis 25yearold s...   \n",
       "\n",
       "                                                text  \\\n",
       "0  u biotech firm novavax ha announced buying cze...   \n",
       "1  twitter billionaire ceo jack dorsey ha donated...   \n",
       "2  google exploring investment vodafone idea part...   \n",
       "3  microsoft talk mukesh ambaniled reliance indus...   \n",
       "4  asia richest person mukesh ambanis yearold son...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  biotech firm novavax announced buying czech re...  \n",
       "1  twitter billionaire ceo jack dorsey donated mi...  \n",
       "2  google exploring investment vodafone idea part...  \n",
       "3  microsoft talk mukesh ambaniled reliance indus...  \n",
       "4  asia richest person mukesh ambanis yearold son...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove extra stopwords identified through exploration using the function with extra_words\n",
    "\n",
    "extra_words = ['', 'ha', 'wa', 'said', 'u']\n",
    "df = remove_stopwords(df, 'text', extra_words=extra_words)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>biotech firm novavax announced buying czech re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  business  biotech firm novavax announced buying czech re..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select label and text columns and rename accordingly\n",
    "\n",
    "df_lem = df[['topic', 'clean_text']]\n",
    "\n",
    "df_lem = df_lem.rename(columns={'topic': 'label', 'clean_text': 'text'})\n",
    "\n",
    "# df_lem uses clean, lemmatized text\n",
    "\n",
    "df_lem.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>biotech firm novavax announced buying czech re...</td>\n",
       "      <td>[biotech, firm, novavax, announced, buying, cz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>twitter billionaire ceo jack dorsey donated mi...</td>\n",
       "      <td>[twitter, billionaire, ceo, jack, dorsey, dona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0  business  biotech firm novavax announced buying czech re...   \n",
       "1  business  twitter billionaire ceo jack dorsey donated mi...   \n",
       "\n",
       "                                               words  \n",
       "0  [biotech, firm, novavax, announced, buying, cz...  \n",
       "1  [twitter, billionaire, ceo, jack, dorsey, dona...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a column called words that makes each text observation a list of tokens/words\n",
    "\n",
    "df_lem['words'] = df_lem.text.str.split()\n",
    "df_lem.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Label Column\n",
    "\n",
    "Our dataframe has 100 rows/observations and 3 columns; each observation is the text, a list of tokens/words, and the label of one article.`label` is one of four topics: business, entertainment, technology, and sports, which is our target. `text` is a string of tokens/words from each observation or article. `words` is the text broken into a list of tokens/words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our df_lem dataframe has 100 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "print(f'Our df_lem dataframe has {df_lem.shape[0]} rows and {df_lem.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "entertainment    25\n",
       "sports           25\n",
       "business         25\n",
       "technology       25\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I only have four possible values for my label column. I'm going to make it a category dtype\n",
    "\n",
    "df_lem.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lem['label'] = df_lem.label.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sports</th>\n",
       "      <td>25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>entertainment</th>\n",
       "      <td>25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business</th>\n",
       "      <td>25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                n  percent\n",
       "technology     25     0.25\n",
       "sports         25     0.25\n",
       "entertainment  25     0.25\n",
       "business       25     0.25"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our scraped data is very balanced!\n",
    "\n",
    "labels = pd.concat([df_lem.label.value_counts(),\n",
    "                    df_lem.label.value_counts(normalize=True)], axis=1)\n",
    "labels.columns = ['n', 'percent']\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Numeric Columns from Text\n",
    "\n",
    "We can create some numeric columns from our text. Here are two examples of numeric measures of aspects of our text data that might prove useful to classify our observations in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>biotech firm novavax announced buying czech re...</td>\n",
       "      <td>[biotech, firm, novavax, announced, buying, cz...</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>twitter billionaire ceo jack dorsey donated mi...</td>\n",
       "      <td>[twitter, billionaire, ceo, jack, dorsey, dona...</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>google exploring investment vodafone idea part...</td>\n",
       "      <td>[google, exploring, investment, vodafone, idea...</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>microsoft talk mukesh ambaniled reliance indus...</td>\n",
       "      <td>[microsoft, talk, mukesh, ambaniled, reliance,...</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>asia richest person mukesh ambanis yearold son...</td>\n",
       "      <td>[asia, richest, person, mukesh, ambanis, yearo...</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0  business  biotech firm novavax announced buying czech re...   \n",
       "1  business  twitter billionaire ceo jack dorsey donated mi...   \n",
       "2  business  google exploring investment vodafone idea part...   \n",
       "3  business  microsoft talk mukesh ambaniled reliance indus...   \n",
       "4  business  asia richest person mukesh ambanis yearold son...   \n",
       "\n",
       "                                               words  text_length  \n",
       "0  [biotech, firm, novavax, announced, buying, cz...          274  \n",
       "1  [twitter, billionaire, ceo, jack, dorsey, dona...          260  \n",
       "2  [google, exploring, investment, vodafone, idea...          230  \n",
       "3  [microsoft, talk, mukesh, ambaniled, reliance,...          262  \n",
       "4  [asia, richest, person, mukesh, ambanis, yearo...          245  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a column that gives the length of the text/string for each observation\n",
    "\n",
    "df_lem['text_length'] = df_lem.text.apply(len)\n",
    "df_lem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>text_length</th>\n",
       "      <th>toke_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>biotech firm novavax announced buying czech re...</td>\n",
       "      <td>[biotech, firm, novavax, announced, buying, cz...</td>\n",
       "      <td>274</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>twitter billionaire ceo jack dorsey donated mi...</td>\n",
       "      <td>[twitter, billionaire, ceo, jack, dorsey, dona...</td>\n",
       "      <td>260</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>google exploring investment vodafone idea part...</td>\n",
       "      <td>[google, exploring, investment, vodafone, idea...</td>\n",
       "      <td>230</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>microsoft talk mukesh ambaniled reliance indus...</td>\n",
       "      <td>[microsoft, talk, mukesh, ambaniled, reliance,...</td>\n",
       "      <td>262</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>asia richest person mukesh ambanis yearold son...</td>\n",
       "      <td>[asia, richest, person, mukesh, ambanis, yearo...</td>\n",
       "      <td>245</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text  \\\n",
       "0  business  biotech firm novavax announced buying czech re...   \n",
       "1  business  twitter billionaire ceo jack dorsey donated mi...   \n",
       "2  business  google exploring investment vodafone idea part...   \n",
       "3  business  microsoft talk mukesh ambaniled reliance indus...   \n",
       "4  business  asia richest person mukesh ambanis yearold son...   \n",
       "\n",
       "                                               words  text_length  toke_count  \n",
       "0  [biotech, firm, novavax, announced, buying, cz...          274          36  \n",
       "1  [twitter, billionaire, ceo, jack, dorsey, dona...          260          37  \n",
       "2  [google, exploring, investment, vodafone, idea...          230          30  \n",
       "3  [microsoft, talk, mukesh, ambaniled, reliance,...          262          36  \n",
       "4  [asia, richest, person, mukesh, ambanis, yearo...          245          34  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a column that gives the length of the text/string for each observation\n",
    "\n",
    "df_lem['toke_count'] = df_lem.words.apply(len)\n",
    "df_lem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Important Words in Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='n_grams'></a>\n",
    "### Create and Visualize N-grams\n",
    "\n",
    "In creating a list of words to search my Corpus for the most common uni-, bi-, and trigrams, I see that I have single letters, and truncated verbs that will not add meaning to my exploration. I will definitely go back to my [extra cleaning](#cleaning) section and remove the following stopwords: `['u', 'ha', 'wa', 'said']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['biotech', 'firm', 'novavax', 'announced', 'buying']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of words from the entire corpus\n",
    "\n",
    "words = ' '.join(df_lem.text).split(' ')\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create lists of uni-, bi-, and trigrams by calling the list function on our ngram generator\n",
    "\n",
    "unigrams = list(ngrams(words, 1))\n",
    "bigrams = list(ngrams(words, 2))\n",
    "trigrams = list(ngrams(words, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['unigrams', 'bigrams', 'trigrams'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary holding the uni-, bi-, and trigrams from our Corpus\n",
    "\n",
    "grams_dict = {'unigrams': unigrams, 'bigrams': bigrams, 'trigrams': trigrams}\n",
    "grams_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biotech',), ('firm',), ('novavax',), ('announced',), ('buying',)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out unigrams\n",
    "\n",
    "grams_dict['unigrams'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biotech', 'firm'), ('firm', 'novavax'), ('novavax', 'announced')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out bigrams\n",
    "\n",
    "grams_dict['bigrams'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('biotech', 'firm', 'novavax'), ('firm', 'novavax', 'announced')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out trigrams\n",
    "\n",
    "grams_dict['trigrams'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create N-grams Function\n",
    "\n",
    "I'll create a function to create a dictionary of uni-, bi-, and trigrams for a text column like I just did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_ngrams(df, col):\n",
    "    '''\n",
    "    This function takes in a df and a string name for a column, creates\n",
    "    a list of words from entire corpus, and\n",
    "    returns a dictionary holding all uni-, bi-, and trigrams.\n",
    "    '''\n",
    "    # create our corpus\n",
    "    words = ' '.join(df[col]).split(' ')\n",
    "    \n",
    "    # create lists of uni-, bi-, and trigrams\n",
    "    unigrams = list(ngrams(words, 1))\n",
    "    bigrams = list(ngrams(words, 2))\n",
    "    trigrams = list(ngrams(words, 3))\n",
    "    \n",
    "    # create a dictionary holding the uni-, bi-, and trigrams from our words\n",
    "    grams_dict = {'unigrams': unigrams, 'bigrams': bigrams, 'trigrams': trigrams}\n",
    "    \n",
    "    return grams_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['unigrams', 'bigrams', 'trigrams'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_dict = grab_ngrams(df_lem, 'text')\n",
    "ngrams_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create N-Gram Viz Functions\n",
    "\n",
    "I want to create two functions here. \n",
    "\n",
    "- `ngrams_viz(ngrams_dict, topic)` will take in a dictionary of uni-, bi-, and trigrams and a string for topic and display viz for the top 20 most common of each for a single topic.\n",
    "\n",
    "\n",
    "- `ngrams_compare()` will take in a df with a 'label' column and a 'text' column, \n",
    "    an int: 1 for unigrams, 2 for bigrams, 3 for trigrams,\n",
    "    and a string for color for customization and displays horizontal bar chart\n",
    "    subplots by topic with the top 20 common uni-, bi-, or trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_viz(ngram_dict, topic):\n",
    "    '''\n",
    "    This function takes in a dictionary of uni-, bi-, and trigrams\n",
    "    and displays viz for the top 20 most common of each.\n",
    "    '''\n",
    "    # loop through the key in the dictionary\n",
    "    for key in ngrams_dict:\n",
    "        \n",
    "        # convert value_counts to a Series and plot top 20\n",
    "        (pd.Series(ngrams_dict[key])\n",
    "         .value_counts()\n",
    "         .head(20).plot.barh(color='thistle', width=.9)\n",
    "        )\n",
    "\n",
    "        plt.title('20 Most Common ' + topic + ' ' + key)\n",
    "        \n",
    "        # reorder y-axis of horizontal bar chart\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the top uni-, bi-, and trigrams for all of our words together\n",
    "\n",
    "ngrams_viz(ngrams_dict, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Words by Topic Label\n",
    "\n",
    "Here I will create my bag of words or large combined list of all of the tokens/words from our corpus of documents by topic. In our context, that means that we will join all of the text from each row of a column into a long string of tokens/words, and those strings into lists of individual tokens/words. The difference here from the n-grams we explored above is that I will filter the df by topic for each of our four topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of df for business\n",
    "\n",
    "biz_df = df_lem[df_lem.label == 'business']\n",
    "biz_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of uni-, bi-, and trigrams for biz\n",
    "\n",
    "biz_grams = grab_ngrams(biz_df, 'text')\n",
    "biz_grams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_viz(biz_grams, 'Business')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of df for entertainment\n",
    "\n",
    "entertainment_df = df_lem[df_lem.label == 'entertainment']\n",
    "entertainment_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of uni-, bi-, and trigrams for entertainment\n",
    "\n",
    "ent_grams = grab_ngrams(entertainment_df, 'text')\n",
    "ent_grams.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of df for sports\n",
    "\n",
    "sports_df = df_lem[df_lem.label == 'sports']\n",
    "sports_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of uni-, bi-, and trigrams for sports\n",
    "\n",
    "sports_grams = grab_ngrams(sports_df, 'text')\n",
    "sports_grams.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subset of df for technology\n",
    "\n",
    "tech_df = df_lem[df_lem.label == 'technology']\n",
    "tech_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of uni-, bi-, and trigrams for tech\n",
    "\n",
    "tech_grams = grab_ngrams(tech_df, 'text')\n",
    "tech_grams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_compare(df, num, color):\n",
    "    '''\n",
    "    This function takes in a df with a 'label' column and a 'words' column, \n",
    "    an int: 1 for unigrams, 2 for bigrams, 3 for trigrams,\n",
    "    and a string for color for customization and displays horizontal bar chart\n",
    "    subplots by topic with the top 20 common uni-, bi-, or trigrams.\n",
    "    '''\n",
    "    # create topics list to loop through\n",
    "    topics = df.label.unique()\n",
    "    \n",
    "    # create figure for subplots\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # loop through topics\n",
    "    for i, topic in list(enumerate(topics, start=1)):\n",
    "        \n",
    "        # assign subtopic df to temp_df\n",
    "        temp_df = df[df.label == topic]\n",
    "        \n",
    "        # create a list of words from entire corpus\n",
    "        words = ' '.join(temp_df['text']).split(' ')\n",
    "\n",
    "        # create lists of uni-, bi-, and trigrams\n",
    "        n_grams = list(ngrams(words, num))\n",
    "\n",
    "        subplot(2,2,i)\n",
    "\n",
    "        (pd.Series(n_grams)\n",
    "                .value_counts()\n",
    "                .head(20)\n",
    "                .plot.barh(color=color, width=.9, fontsize=15)\n",
    "        )\n",
    "        \n",
    "        # print customized title\n",
    "        plt.title('20 Most Common ' + topic + ' ' + str(num) + '-grams', fontsize=20)\n",
    "\n",
    "        # reorder y-axis of horizontal bar chart\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "    # display settings    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join(df_lem['text']).split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_compare(df_lem, 1, 'thistle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_compare(df_lem, 2, 'dodgerblue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_compare(df_lem, 3, 'darkorange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Series of Word Frequencies for Each Topic Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all words from corpus from subset biz_df\n",
    "business_words = ' '.join(biz_df.text).split(' ')\n",
    "business_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series of word frequency from our list of business words\n",
    "business_freq = pd.Series(business_words).value_counts()\n",
    "business_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_freq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all words from corpus from subset entertainment_df\n",
    "entertainment_words = ' '.join(entertainment_df.text).split(' ')\n",
    "\n",
    "# Create a Series of word frequency from our list of entertainment words\n",
    "entertainment_freq = pd.Series(entertainment_words).value_counts()\n",
    "entertainment_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entertainment_freq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all words from corpus from subset entertainment_df\n",
    "sports_words = ' '.join(sports_df.text).split(' ')\n",
    "\n",
    "# Create a Series of word frequency from our list of sports words\n",
    "sports_freq = pd.Series(sports_words).value_counts()\n",
    "sports_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_freq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all words from corpus from subset tech_df\n",
    "tech_words = ' '.join(tech_df.text).split(' ')\n",
    "\n",
    "# Create a Series of word frequency from our list of tech words\n",
    "technology_freq = pd.Series(tech_words).value_counts()\n",
    "technology_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "technology_freq.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all words from corpus\n",
    "all_words = ' '.join(df_lem.text).split(' ')\n",
    "\n",
    "# Create a Series of word frequency from our list of all words\n",
    "all_freq = pd.Series(all_words).value_counts()\n",
    "all_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_freq.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create df of Word Frequencies for Each Subset Above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = (pd.concat([business_freq, entertainment_freq, sports_freq, technology_freq, all_freq], axis=1, sort=True)\n",
    "              .set_axis(['all', 'business', 'entertainment', 'sports', 'technology'], axis=1, inplace=False)\n",
    "              .fillna(0)\n",
    "              .apply(lambda s: s.astype(int)))\n",
    "\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values(by='sports', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Topic-specific Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts[word_counts.business == 0].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='word_clouds'></a>\n",
    "### Word Clouds!\n",
    "\n",
    "**Word Clouds are a great way to visualize word importance or word frequency in your text data.** The default word cloud background is black, which can look pretty cool, but there are many, many ways to customize not only the look but the functionality of your word clouds.\n",
    "\n",
    "Here, we'll look at word frequency for the subsets of business articles and technology articles. We will also look at a word cloud that visualizes the frequency of the top 20 most common bigrams for the business subset.\n",
    "\n",
    "**<font color=purple>These examples should give you a great start with customizing word clouds to visualize your text data.</font>**\n",
    "\n",
    "**Step 1: Import Libraries.**\n",
    "\n",
    "```python\n",
    "python -m pip install --upgrade wordcloud\n",
    "```\n",
    "\n",
    "```python\n",
    "from wordcloud import WordCloud\n",
    "```\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "**Step 2: Create a list of words for your cloud, in this case from your text column.**\n",
    "\n",
    "```python\n",
    "words = ' '.join(df['text']).split(' ')\n",
    "```\n",
    "\n",
    "**Step 3: Create the WordCloud Object.**\n",
    "\n",
    "```python\n",
    "wordcloud = WordCloud().generate(words)\n",
    "```\n",
    "\n",
    "**Step 4: Display the Image.**\n",
    "\n",
    "```python\n",
    "plt.imshow(wordcloud)\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.axis('off')\n",
    "```\n",
    "\n",
    "```python\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standard Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "technology_cloud = WordCloud().generate(' '.join(tech_words))\n",
    "\n",
    "plt.imshow(technology_cloud)\n",
    "plt.axis('off')\n",
    "plt.title('Technology Article Words', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "business_cloud = WordCloud(background_color='white', colormap=plt.cm.Dark2,\n",
    "                           height=1000, width=1400).generate(' '.join(business_words))\n",
    "\n",
    "plt.imshow(business_cloud)\n",
    "plt.axis('off')\n",
    "plt.title('Business Article Words', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Series of frequency of our top 20 business bigrams\n",
    "\n",
    "top_20_biz_bigrams = pd.Series(biz_grams['bigrams']).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary in which the keys are the bigrams tuples and the values the frequencies of bigrams\n",
    "\n",
    "biz_bigram_dict = top_20_biz_bigrams.to_dict().items()\n",
    "biz_bigram_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary comprehension to return a dictionary with bigram as key and frequency as value\n",
    "\n",
    "data = {key[0] + ' ' + key[1]: val for key, val in biz_bigram_dict}\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display bigram word cloud\n",
    "\n",
    "business_bigram_cloud = WordCloud(background_color='white', colormap=plt.cm.tab10,\n",
    "                           height=1000, width=1400).generate_from_frequencies(data)\n",
    "\n",
    "plt.imshow(business_bigram_cloud)\n",
    "plt.axis('off')\n",
    "plt.title('Business Article Top 20 Bigrams', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shaped Word Cloud\n",
    "\n",
    "I found [this article](https://www.datacamp.com/community/tutorials/wordcloud-python) really helpful to understand how to create my masks to shape my word clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need this import\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and check mask using saved png file in my directory\n",
    "\n",
    "b_mask = np.array(Image.open('b.png'))\n",
    "b_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform the white part of the mask to 255 (integer), pure white\n",
    "\n",
    "def transform_format(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    if val == 1:\n",
    "        return 255\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform mask into a new one that will work with the function:\n",
    "\n",
    "transformed_b_mask = np.ndarray((b_mask.shape[0],b_mask.shape[1]), np.int32)\n",
    "\n",
    "for i in range(len(b_mask)):\n",
    "    transformed_b_mask[i] = list(map(transform_format, b_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_shape_cloud = WordCloud(background_color='white', colormap=plt.cm.Dark2,\n",
    "                                   mask=transformed_b_mask, contour_width=3, contour_color='black',\n",
    "                                  height=1000, width=1400).generate(' '.join(business_words))\n",
    "\n",
    "# store to file\n",
    "#bayes_background_cloud.to_file(\"bayes_backgrounds.png\")\n",
    "\n",
    "# show cloud\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(biz_shape_cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "200.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
