{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Beautiful-Soup---Web-Scraping\" data-toc-modified-id=\"Beautiful-Soup---Web-Scraping-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Beautiful Soup - Web Scraping</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-Beautiful-Soup?\" data-toc-modified-id=\"What-is-Beautiful-Soup?-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span><strong><font color=\"red\">What is Beautiful Soup?</font></strong></a></span></li><li><span><a href=\"#So-What-Do-I-Do-With-Beautiful-Soup?\" data-toc-modified-id=\"So-What-Do-I-Do-With-Beautiful-Soup?-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span><strong><font color=\"orange\">So What Do I Do With Beautiful Soup?</font></strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#HTML-Elements\" data-toc-modified-id=\"HTML-Elements-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span><strong><font color=\"purple\">HTML Elements</font></strong></a></span></li><li><span><a href=\"#HTML-Terms\" data-toc-modified-id=\"HTML-Terms-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span><strong><font color=\"purple\">HTML Terms</font></strong></a></span></li><li><span><a href=\"#HTML-Properties-and-Attributes\" data-toc-modified-id=\"HTML-Properties-and-Attributes-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span><strong><font color=\"purple\">HTML Properties and Attributes</font></strong></a></span></li></ul></li><li><span><a href=\"#Now-What?\" data-toc-modified-id=\"Now-What?-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span><strong><font color=\"green\">Now What?</font></strong></a></span></li><li><span><a href=\"#Codeup-Blogs\" data-toc-modified-id=\"Codeup-Blogs-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span><strong><font color=\"blue\">Codeup Blogs</font></strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Grab-Title-from-Page\" data-toc-modified-id=\"Grab-Title-from-Page-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span><strong><font color=\"purple\">Grab Title from Page</font></strong></a></span></li><li><span><a href=\"#Grab-Text-from-Page\" data-toc-modified-id=\"Grab-Text-from-Page-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span><strong><font color=\"purple\">Grab Text from Page</font></strong></a></span></li><li><span><a href=\"#Build-Blog-Function\" data-toc-modified-id=\"Build-Blog-Function-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span><strong><font color=\"purple\">Build Blog Function</font></strong></a></span></li><li><span><a href=\"#Test-Function\" data-toc-modified-id=\"Test-Function-1.4.4\"><span class=\"toc-item-num\">1.4.4&nbsp;&nbsp;</span><strong><font color=\"purple\">Test Function</font></strong></a></span></li><li><span><a href=\"#Bonus-URL-Scrape\" data-toc-modified-id=\"Bonus-URL-Scrape-1.4.5\"><span class=\"toc-item-num\">1.4.5&nbsp;&nbsp;</span><strong><font color=\"purple\">Bonus URL Scrape</font></strong></a></span></li><li><span><a href=\"#Bonus-URL-Function\" data-toc-modified-id=\"Bonus-URL-Function-1.4.6\"><span class=\"toc-item-num\">1.4.6&nbsp;&nbsp;</span><strong><font color=\"purple\">Bonus URL Function</font></strong></a></span></li></ul></li><li><span><a href=\"#Inshorts-News-Articles\" data-toc-modified-id=\"Inshorts-News-Articles-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span><strong><font color=\"blue\">Inshorts News Articles</font></strong></a></span><ul class=\"toc-item\"><li><span><a href=\"#Scrape-and-Get-Internal-URLS-from-Article-Page\" data-toc-modified-id=\"Scrape-and-Get-Internal-URLS-from-Article-Page-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span><strong><font color=\"purple\">Scrape and Get Internal URLS from Article Page</font></strong></a></span></li><li><span><a href=\"#Scrape-Article-News-Cards\" data-toc-modified-id=\"Scrape-Article-News-Cards-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span><strong><font color=\"purple\">Scrape Article News Cards</font></strong></a></span></li><li><span><a href=\"#Scrape-Author-from-Article-Page\" data-toc-modified-id=\"Scrape-Author-from-Article-Page-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span><strong><font color=\"purple\">Scrape Author from Article Page</font></strong></a></span></li><li><span><a href=\"#Scrape-Text-from-Article-Page\" data-toc-modified-id=\"Scrape-Text-from-Article-Page-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span><strong><font color=\"purple\">Scrape Text from Article Page</font></strong></a></span></li><li><span><a href=\"#Scrape-Source-URL-from-Article-Page\" data-toc-modified-id=\"Scrape-Source-URL-from-Article-Page-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span><strong><font color=\"purple\">Scrape Source URL from Article Page</font></strong></a></span></li><li><span><a href=\"#Build-Article-Function\" data-toc-modified-id=\"Build-Article-Function-1.5.6\"><span class=\"toc-item-num\">1.5.6&nbsp;&nbsp;</span><strong><font color=\"purple\">Build Article Function</font></strong></a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from requests import get\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup - Web Scraping\n",
    "\n",
    "#### **<font color=red>What is Beautiful Soup?</font>**\n",
    "\n",
    "\n",
    "\n",
    "#### **<font color=orange>So What Do I Do With Beautiful Soup?</font>**\n",
    "\n",
    "Here, we are looking to retrieve content from a web page, but the web page is written in HTML (HyperText Markup Language), so we need a basic understanding of the different HTML elements used to create web pages.\n",
    "\n",
    "##### **<font color=purple>HTML Elements</font>**\n",
    "\n",
    "`<html>` tag identifies all contents as html\n",
    "\n",
    "`<head>` tag contains data about the title of the web page\n",
    "\n",
    "`<body>` tag contains the main content of the web page\n",
    "\n",
    "`<p>` or paragraph tag creates new pargraphs of text\n",
    "\n",
    "`<a>` tag (tell the browser to render a link). `<a>` tags use the `href` property to tell the link where to go.\n",
    "\n",
    "`<div>` tag indentifies a division of the page\n",
    "\n",
    "`<b>` tag bolds the text inside\n",
    "\n",
    "`<i>` tag italicizes the text inside\n",
    "\n",
    "`<table>` tag creates a table\n",
    "\n",
    "`<form>` tag creates an input form\n",
    "   \n",
    "><font color=purple>The tags are nested inside of the main html tag like you see below.</font>\n",
    "\n",
    "```html\n",
    "<html>\n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "    <p>\n",
    "        <a href = \"link\", id='name_of_link'>\n",
    "    </p>\n",
    "    <p class=''>\n",
    "    </p>\n",
    "    </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "##### **<font color=purple>HTML Terms</font>**\n",
    "\n",
    "**child** -> a tag inside of another tag. The `<p>` tags above are children of the `<body>` tag.\n",
    "\n",
    "**parent** -> the tag another tag is inside of. The `<body>` tag is the parent of the `<p>` tag.\n",
    "\n",
    "**sibling** -> a tag that is nested inside the same parent tag as another tag. The two `<p>` tags are siblings tags inside of the `<body>` tag.\n",
    "\n",
    "##### **<font color=purple>HTML Properties and Attributes</font>**\n",
    "\n",
    "These are optional, but they make the HTML elements easier to work with because they give the elements names. You will have to examine a web page to find out if it uses these properties.\n",
    "\n",
    "`class` is a property of an HTML element. One element can have multiple classes and elements can share the same classes, so classes cannot be used as unique identifiers.\n",
    "\n",
    "`id` is a property of an HTML element. Each element can only have one id, so they can be used as unique identifiers.\n",
    "\n",
    "`itemprop` is an attribute that consists of a name-value pair and is used to add properties to an item.\n",
    "\n",
    "#### **<font color=green>Now What?</font>**\n",
    "\n",
    "We will need to use the `requests` library to retrieve the HTML from a web page we want to scrape. You can review how to use the `requests` library in my notebook [here](https://faithkane3.github.io/time_series_review/time_series_review).\n",
    "\n",
    "Next, we will inspect the structure of the web page by right-clicking on the page we want to scrape and clicking 'inspect'. This will allow us to move our cursor over the part of the page we want to scrape and see the responsible HTML code for that section high-lighted on the right. We can use the tag and its properties with `BeautifulSoup` to `soup.find(name=tag)`\n",
    "\n",
    "We will need to use `BeautifulSoup` to parse the HTML response to our request. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeup.com/codeups-data-science-career-accelerator-is-here/'\n",
    "headers = {'User-Agent': 'Codeup Bayes Data Science'} \n",
    "    \n",
    "response = get(url, headers=headers)\n",
    "response.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's our long string; we'll use this to make our soup object\n",
    "\n",
    "print(type(response.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup using our response string\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Now we have our BeautifulSoup object, we can use its built-in methods and properties\n",
    "\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<font color=blue>Codeup Blogs</font>**\n",
    "\n",
    "**Goals:** Write a function to scrape urls from main Codeup blog web page and write a function that returns a dictionary of blog titles and text for each blog page. \n",
    "\n",
    "##### **<font color=purple>Grab Title from Page</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use the find method on my soup passing in \n",
    "\n",
    "title = soup.find('h1', itemprop='headline' ).get_text()\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Grab Text from Page</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = soup.find('div', itemprop='text').get_text()\n",
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(article))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Build Blog Function</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = ['https://codeup.com/codeups-data-science-career-accelerator-is-here/',\n",
    "            'https://codeup.com/data-science-myths/',\n",
    "            'https://codeup.com/data-science-vs-data-analytics-whats-the-difference/',\n",
    "            'https://codeup.com/10-tips-to-crush-it-at-the-sa-tech-job-fair/',\n",
    "            'https://codeup.com/competitor-bootcamps-are-closing-is-the-model-in-danger/']\n",
    "\n",
    "def get_blog_articles(urls):\n",
    "    '''\n",
    "    This function takes in a list of Codeup Blog urls and a dictionary headers \n",
    "    and scrapes the title and text for each url returning a list of dictionaries\n",
    "    with the title and text for each blog.\n",
    "    '''\n",
    "    headers = {'User-Agent': 'Codeup Bayes Data Science'} \n",
    "    \n",
    "    # Create an empty list to hold dictionaries\n",
    "    articles = []\n",
    "    \n",
    "    # Loop through each url in our list of urls\n",
    "    for url in urls:\n",
    "        \n",
    "        # get request to each url saved in response\n",
    "        response = get(url, headers=headers)\n",
    "        \n",
    "        # Create soup object from response text and parse\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Save the title of each blog in variable title\n",
    "        title = soup.find('h1', itemprop='headline' ).get_text()\n",
    "        \n",
    "        # Save the text in each blog to variable text\n",
    "        text = soup.find('div', itemprop='text').get_text()\n",
    "        \n",
    "        # Create a dictionary holding the title and text for each blog\n",
    "        article = {'title': title, 'content': text}\n",
    "        \n",
    "        # Add each dictionary to the articles list of dictionaries\n",
    "        articles.append(article)\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Test Function</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_blog_articles(urls=urls)\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Bonus URL Scrape</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to hit Codeup's main blog page to scarpe the urls\n",
    "\n",
    "url = 'https://codeup.com/resources/#blog'\n",
    "headers = {'User-Agent': 'Codeup Bayes Data Science'} \n",
    "\n",
    "response = get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm using class_ to get a list of tag elements from my soup object\n",
    "\n",
    "link_list = soup.find_all(class_=\"jet-listing-dynamic-link__link\")\n",
    "link_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(link_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty urls list and for each chunk above, grab the href/link\n",
    "# Add each link to the urls list\n",
    "\n",
    "urls = []\n",
    "for link in link_list:\n",
    "    urls.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wow, 99 links! Ready to scrape titles and text from each\n",
    "\n",
    "print(len(urls))\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Bonus URL Function</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls():\n",
    "     '''\n",
    "    This function scrapes all of the Codeup blog urls from\n",
    "    the main Codeup blog page and returns a list of urls.\n",
    "    '''\n",
    "    # The main Codeup blog page with all the urls\n",
    "    url = 'https://codeup.com/resources/#blog'\n",
    "    \n",
    "    headers = {'User-Agent': 'Codeup Bayes Data Science'} \n",
    "    \n",
    "    # Send request to main page and get response\n",
    "    response = get(url, headers=headers)\n",
    "    \n",
    "    # Create soup object using response\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Create empty list to hold the urls for all blogs\n",
    "    urls = []\n",
    "    \n",
    "    # Create a list of the element tags that hold the href/links\n",
    "    link_list = soup.find_all(class_=\"jet-listing-dynamic-link__link\")\n",
    "    \n",
    "    # get the href/link from each element tag in my list\n",
    "    for link in link_list:\n",
    "        \n",
    "        # Add the link to my urls list\n",
    "        urls.append(link.get('href'))\n",
    "        \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I can use my same function with my new urls list!\n",
    "\n",
    "big_articles = acquire_blog_articles(urls=get_all_urls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(big_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<font color=blue>Inshorts News Articles</font>**\n",
    "\n",
    "**Goal:**  Write a function that scrapes the news articles for the following topics:\n",
    "\n",
    "- Business\n",
    "\n",
    "\n",
    "- Sports\n",
    "\n",
    "\n",
    "- Technology\n",
    "\n",
    "\n",
    "- Entertainment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://inshorts.com/en/read/business'\n",
    "\n",
    "response = get(url)\n",
    "response.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Scrape and Get Internal URLS from Article Page</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://inshorts.com/en/news/delhimumbai-tickets-to-cost-₹3500₹10000-air-routes-divided-into-7-sections-1590056906812',\n",
       " 'https://inshorts.com/en/news/govt-releases-fare-structure-for-domestic-flights-for-different-flight-durations-1590071145249']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This scrapes the main page for urls to access cards\n",
    "\n",
    "cards = soup.find_all(class_='news-card-title')\n",
    "\n",
    "base_url = 'https://inshorts.com'\n",
    "card_urls = []\n",
    "for card in cards:\n",
    "    card_urls.append(base_url + card.a['href'])\n",
    "\n",
    "print(len(card_urls))\n",
    "card_urls[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Scrape Article News Cards</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<div class=\"news-card z-depth-1\" itemscope=\"\" itemtype=\"http://schema.org/NewsArticle\">\n",
       "<span content=\"\" itemid=\"https://inshorts.com/en/news/delhimumbai-tickets-to-cost-₹3500₹10000-air-routes-divided-into-7-sections-1590056906812\" itemprop=\"mainEntityOfPage\" itemscope=\"\" itemtype=\"https://schema.org/WebPage\"></span>\n",
       "<span itemprop=\"author\" itemscope=\"itemscope\" itemtype=\"https://schema.org/Person\">\n",
       "<span content=\"Krishna Veera Vanamali\" itemprop=\"name\"></span>\n",
       "</span>\n",
       "<span content=\"Delhi-Mumbai tickets to cost ₹3,500-₹10,000, air routes divided into 7 sections\" itemprop=\"description\"></span>\n",
       "<span itemprop=\"image\" itemscope=\"\" itemtype=\"https://schema.org/ImageObject\">\n",
       "<meta content=\"https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2020/05_may/21_thu/img_1590056568888_954.jpg?\" itemprop=\"url\"/>\n",
       "<meta content=\"864\" itemprop=\"width\"/>\n",
       "<meta content=\"483\" itemprop=\"height\"/>\n",
       "</span>\n",
       "<span itemprop=\"publisher\" itemscope=\"itemscope\" itemtype=\"https://schema.org/Organization\">\n",
       "<span content=\"https://inshorts.com/\" itemprop=\"url\"></span>\n",
       "<span content=\"Inshorts\" itemprop=\"name\"></span>\n",
       "<span itemprop=\"logo\" itemscope=\"\" itemtype=\"https://schema.org/ImageObject\">\n",
       "<span content=\"https://assets.inshorts.com/inshorts/images/v1/variants/jpg/m/2018/11_nov/21_wed/img_1542823931298_497.jpg\" itemprop=\"url\"></span>\n",
       "<meta content=\"400\" itemprop=\"width\"/>\n",
       "<meta content=\"60\" itemprop=\"height\"/>\n",
       "</span>\n",
       "</span>\n",
       "<div class=\"news-card-image\" style=\"background-image: url('https://static.inshorts.com/inshorts/images/v1/variants/jpg/m/2020/05_may/21_thu/img_1590056568888_954.jpg?')\">\n",
       "</div>\n",
       "<div class=\"news-card-title news-right-box\">\n",
       "<a class=\"clickable\" href=\"/en/news/delhimumbai-tickets-to-cost-₹3500₹10000-air-routes-divided-into-7-sections-1590056906812\" onclick=\"ga('send', {'hitType': 'event', 'eventCategory': 'TitleOfNews', 'eventAction': 'clicked', 'eventLabel': 'Delhi-Mumbai%20tickets%20to%20cost%20%E2%82%B93%2C500-%E2%82%B910%2C000%2C%20air%20routes%20divided%20into%207%20sections)' });\" style=\"color:#44444d!important\">\n",
       "<span itemprop=\"headline\">Delhi-Mumbai tickets to cost ₹3,500-₹10,000, air routes divided into 7 sections</span>\n",
       "</a>\n",
       "<div class=\"news-card-author-time news-card-author-time-in-title\">\n",
       "<a href=\"/prev/en/news/delhimumbai-tickets-to-cost-₹3500₹10000-air-routes-divided-into-7-sections-1590056906812\"><span class=\"short\">short</span></a> by <span class=\"author\">Krishna Veera Vanamali</span> / \n",
       "      <span class=\"time\" content=\"2020-05-21T10:28:26.000Z\" itemprop=\"datePublished\">03:58 pm</span> on <span clas=\"date\">21 May 2020,Thursday</span>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"news-card-content news-right-box\">\n",
       "<div itemprop=\"articleBody\">The government has divided air routes into 7 categories based on flight durations for regulating fares till August 24. For a journey between 90-120 minutes, which includes the Delhi-Mumbai route, the minimum fare will be ₹3,500 and maximum ₹10,000. 40% of the tickets will have to be sold at a price less than the mid-point of the price band.</div>\n",
       "<div class=\"news-card-author-time news-card-author-time-in-content\">\n",
       "<a href=\"/prev/en/news/delhimumbai-tickets-to-cost-₹3500₹10000-air-routes-divided-into-7-sections-1590056906812\"><span class=\"short\">short</span></a> by <span class=\"author\">Krishna Veera Vanamali</span> / \n",
       "      <span class=\"time\" content=\"2020-05-21T10:28:26.000Z\" itemprop=\"dateModified\">03:58 pm</span> on <span class=\"date\">21 May</span>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"news-card-footer news-right-box\">\n",
       "<div class=\"read-more\">read more at <a class=\"source\" href=\"https://www.deccanherald.com/national/coronavirus-in-india-news-live-updates-total-cases-deaths-covid-19-tracker-today-worldometer-update-lockdown-40-latest-news-838583.html?utm_campaign=fullarticle&amp;utm_medium=referral&amp;utm_source=inshorts \" onclick=\"ga('send', {'hitType': 'event', 'eventCategory': 'ReadMore', 'eventAction': 'clicked', 'eventLabel': 'Deccan%20Herald' });\" target=\"_blank\">Deccan Herald</a></div>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = soup.find_all(class_='news-card')\n",
    "print(len(articles))\n",
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for article in articles:\n",
    "    print(article.select(\"[itemprop='author']\")[0].get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Delhi-Mumbai tickets to cost ₹3,500-₹10,000, air routes divided into 7 sections'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the title of each article to a list of titles\n",
    "\n",
    "title = soup.find('span', itemprop='headline' ).get_text()\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Scrape Author from Article Page</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the author of each article to variable author\n",
    "\n",
    "author = soup.find('span', class_='author').get_text()\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Scrape Text from Article Page</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the text of each article to variable text\n",
    "\n",
    "text = soup.find('div', itemprop='articleBody').get_text()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Scrape Source URL from Article Page</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the url of each article to variable scource_url\n",
    "\n",
    "source_url = soup.find(class_='source').get('href')\n",
    "source_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **<font color=purple>Build Article Function</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_newscard_urls():\n",
    "    '''\n",
    "    This function gets a list of urls from the main\n",
    "    page of the inshort card preview page.\n",
    "    '''\n",
    "    #\n",
    "    topics = ['business', 'sports', 'technology', 'entertainment']\n",
    "    base_url = 'https://inshorts.com/en/read/'\n",
    "    # Get a response object from the main inshorts page\n",
    "    url = 'https://inshorts.com/en/read/'\n",
    "    response = get(url)\n",
    "    \n",
    "    # Create soup object using response from inshort\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Create list of news card urls within inshort\n",
    "    cards = soup.find_all(class_='news-card-title')\n",
    "    \n",
    "    base_url = 'https://inshorts.com'\n",
    "    urls = []\n",
    "    for card in cards:\n",
    "        urls.append(base_url + card.a['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_newscard_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_articles():\n",
    "    '''\n",
    "    This function takes in a list of Inshort article urls\n",
    "    and scrapes the title and text for each url returning a list of dictionaries\n",
    "    with the title and text for each blog.\n",
    "    '''\n",
    "    # Use helper function to get urls\n",
    "    urls = get_newscard_urls()\n",
    "    \n",
    "    # Create an empty list to hold dictionaries\n",
    "    articles = []\n",
    "    \n",
    "    # Loop through each url in our list of urls\n",
    "    for url in urls:\n",
    "        \n",
    "        # get request to each url saved in response\n",
    "        response = get(url, headers=headers)\n",
    "        \n",
    "        # Create soup object from response text and parse\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Save the title of each article to variable title\n",
    "        title = soup.find('span', itemprop='headline' ).get_text()\n",
    "        \n",
    "        # Save the author of each article to variable author\n",
    "        author = soup.find('span', class_='author' ).get_text()\n",
    "        \n",
    "        # Save the text of each article to variable text\n",
    "        text = soup.find('div', itemprop='articleBody').get_text()\n",
    "        \n",
    "        # Save the url of each article to variable scource_url\n",
    "        source_url = soup.find(class_='source').get('href')\n",
    "        \n",
    "        # Create a dictionary holding the title and text for each blog\n",
    "        article = {'title': title, 'author': author, 'content': text, 'source_url': source_url}\n",
    "        \n",
    "        # Add each dictionary to the articles list of dictionaries\n",
    "        articles.append(article)\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_news_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['business', 'sports', 'technology', 'entertainment']\n",
    "base_url = 'https://inshorts.com/en/read/'\n",
    "\n",
    "response = get(base_url + 'business')\n",
    "response.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our soup object using our response.text\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Create a list of tags, so I can grab the href/link\n",
    "link_list = soup.find_all(class_='source')\n",
    "link_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty urls list and for each chunk above, grab the href/link\n",
    "# Add each link to the urls list\n",
    "\n",
    "urls = []\n",
    "for link in link_list:\n",
    "    urls.append(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have 24 links on the first page of business articles\n",
    "\n",
    "print(len(urls))\n",
    "urls[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(class_='load-more-wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
