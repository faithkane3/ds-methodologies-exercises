{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark.sql.functions import col, expr, lit, min, max, concat, sum, avg, count, mean\n",
    "\n",
    "\n",
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Dataframe Basics\n",
    "\n",
    "- Use the starter code above to create a pandas dataframe.\n",
    "\n",
    "\n",
    "- Convert the pandas dataframe to a spark dataframe. From this point forward, do all of your work with the spark dataframe, not the pandas dataframe.\n",
    "\n",
    "\n",
    "- Show the first 3 rows of the dataframe.\n",
    "\n",
    "\n",
    "- Show the first 7 rows of the dataframe.\n",
    "\n",
    "\n",
    "- View a summary of the data using .describe.\n",
    "\n",
    "\n",
    "- Use .select to create a new dataframe with just the n and abool columns. View the first 5 rows of this dataframe.\n",
    "\n",
    "\n",
    "- Use .select to create a new dataframe with just the group and abool columns. View the first 5 rows of this dataframe.\n",
    "\n",
    "\n",
    "- Use .select to create a new dataframe with the group column and the abool column renamed to a_boolean_value. Show the first 3 rows of this dataframe.\n",
    "\n",
    "\n",
    "- Use .select to create a new dataframe with the group column and the n column renamed to a_numeric_value. Show the first 6 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of df\n",
    "\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----+\n",
      "|summary|                 n|group|\n",
      "+-------+------------------+-----+\n",
      "|  count|                20|   20|\n",
      "|   mean|0.3664026449885217| null|\n",
      "| stddev|0.8905322898155363| null|\n",
      "|    min|-1.261605945319069|    x|\n",
      "|    max|2.1503829673811126|    z|\n",
      "+-------+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n, df.abool).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|group|abool|\n",
      "+-----+-----+\n",
      "|    z|false|\n",
      "|    x|false|\n",
      "|    z|false|\n",
      "|    y|false|\n",
      "|    z|false|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group, df.abool).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "|group|a_boolean_value|\n",
      "+-----+---------------+\n",
      "|    z|          false|\n",
      "|    x|          false|\n",
      "|    z|          false|\n",
      "+-----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group, df.abool.alias('a_boolean_value')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|group|     a_numeric_value|\n",
      "+-----+--------------------+\n",
      "|    z|  -0.712390662050588|\n",
      "|    x|   0.753766378659703|\n",
      "|    z|-0.04450307833805...|\n",
      "|    y| 0.45181233874578974|\n",
      "|    z|  1.3451017084510097|\n",
      "|    y|  0.5323378882945463|\n",
      "+-----+--------------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group, df.n.alias('a_numeric_value')).show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column Manipulation\n",
    "\n",
    "- Use the starter code above to re-create a spark dataframe. Store the spark dataframe in a varaible named df\n",
    "\n",
    "\n",
    "- Use .select to add 4 to the n column. Show the results.\n",
    "\n",
    "\n",
    "- Subtract 5 from the n column and view the results.\n",
    "\n",
    "\n",
    "- Multiply the n column by 2. View the results along with the original numbers.\n",
    "\n",
    "\n",
    "- Add a new column named n2 that is the n value multiplied by -1. Show the first 4 rows of your dataframe. You should see the original n value as well as n2.\n",
    "\n",
    "\n",
    "- Add a new column named n3 that is the n value squared. Show the first 5 rows of your dataframe. You should see both n, n2, and n3.\n",
    "\n",
    "\n",
    "- What happens when you run the code below?\n",
    "\n",
    "        df.group + df.abool\n",
    "        \n",
    "        \n",
    "- What happens when you run the code below? What is the difference between this and the previous code sample?\n",
    "\n",
    "        df.select(df.group + df.abool)\n",
    "\n",
    "\n",
    "- Try adding various other columns together. What are the results of combining the different data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(13)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    {\n",
    "        \"n\": np.random.randn(20),\n",
    "        \"group\": np.random.choice(list(\"xyz\"), 20),\n",
    "        \"abool\": np.random.choice([True, False], 20),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|           (n + 4)|\n",
      "+------------------+\n",
      "|3.2876093379494122|\n",
      "| 4.753766378659703|\n",
      "|3.9554969216619464|\n",
      "|  4.45181233874579|\n",
      "|5.3451017084510095|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n + 4).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|            (n - 5)|\n",
      "+-------------------+\n",
      "| -5.712390662050588|\n",
      "| -4.246233621340297|\n",
      "| -5.044503078338053|\n",
      "|  -4.54818766125421|\n",
      "|-3.6548982915489905|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n - 5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   n|             (n * 2)|\n",
      "+--------------------+--------------------+\n",
      "|  -0.712390662050588|  -1.424781324101176|\n",
      "|   0.753766378659703|   1.507532757319406|\n",
      "|-0.04450307833805...|-0.08900615667610691|\n",
      "| 0.45181233874578974|  0.9036246774915795|\n",
      "|  1.3451017084510097|  2.6902034169020195|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.n, df.n * 2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   n|            (n * -1)|\n",
      "+--------------------+--------------------+\n",
      "|  -0.712390662050588|   0.712390662050588|\n",
      "|   0.753766378659703|  -0.753766378659703|\n",
      "|-0.04450307833805...|0.044503078338053455|\n",
      "| 0.45181233874578974|-0.45181233874578974|\n",
      "+--------------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n2 = col('n') * -1\n",
    "df.select(df.n, n2).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                   n|                  n2|\n",
      "+--------------------+--------------------+\n",
      "|  -0.712390662050588|   0.712390662050588|\n",
      "|   0.753766378659703|  -0.753766378659703|\n",
      "|-0.04450307833805...|0.044503078338053455|\n",
      "| 0.45181233874578974|-0.45181233874578974|\n",
      "|  1.3451017084510097| -1.3451017084510097|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with expr\n",
    "\n",
    "df.select(\n",
    "    expr('n'),\n",
    "    expr('n * -1 AS n2')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   n|            (n * -1)|         POWER(n, 2)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  -0.712390662050588|   0.712390662050588|   0.507500455376875|\n",
      "|   0.753766378659703|  -0.753766378659703|  0.5681637535977627|\n",
      "|-0.04450307833805...|0.044503078338053455|0.001980523981562...|\n",
      "| 0.45181233874578974|-0.45181233874578974| 0.20413438944294027|\n",
      "|  1.3451017084510097| -1.3451017084510097|  1.8092986060778251|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n3 = col('n') ** 2\n",
    "df.select(df.n, n2, n3).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\"\\nmismatched input '2' expecting <EOF>(line 1, pos 5)\\n\\n== SQL ==\\nn ** 2 AS n3\\n-----^^^\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.functions.expr.\n: org.apache.spark.sql.catalyst.parser.ParseException: \nmismatched input '2' expecting <EOF>(line 1, pos 5)\n\n== SQL ==\nn ** 2 AS n3\n-----^^^\n\n\tat org.apache.spark.sql.catalyst.parser.ParseException.withCommand(ParseDriver.scala:241)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parse(ParseDriver.scala:117)\n\tat org.apache.spark.sql.execution.SparkSqlParser.parse(SparkSqlParser.scala:48)\n\tat org.apache.spark.sql.catalyst.parser.AbstractSqlParser.parseExpression(ParseDriver.scala:44)\n\tat org.apache.spark.sql.functions$.expr(functions.scala:1366)\n\tat org.apache.spark.sql.functions.expr(functions.scala)\n\tat sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-6fa33627b3b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n * -1 AS n2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n ** 2 AS n3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m ).show(5)\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mexpr\u001b[0;34m(str)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \"\"\"\n\u001b[1;32m    674\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.parser.ParseException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.streaming.StreamingQueryException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStreamingQueryException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParseException\u001b[0m: \"\\nmismatched input '2' expecting <EOF>(line 1, pos 5)\\n\\n== SQL ==\\nn ** 2 AS n3\\n-----^^^\\n\""
     ]
    }
   ],
   "source": [
    "# with expr\n",
    "\n",
    "df.select(\n",
    "    expr('n'), \n",
    "    expr('n * -1 AS n2'),\n",
    "    expr('n ** 2 AS n3')\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|group|abool_diff|\n",
      "+-----+----------+\n",
      "|    z|     false|\n",
      "|    x|     false|\n",
      "|    z|     false|\n",
      "+-----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another way to rename cols\n",
    "\n",
    "df.selectExpr('group as group','abool as abool_diff').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'(group + abool)'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.group + df.abool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----+\n",
      "|                 n|group|abool|\n",
      "+------------------+-----+-----+\n",
      "|-0.712390662050588|    z|false|\n",
      "| 0.753766378659703|    x|false|\n",
      "+------------------+-----+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|(group + n)|\n",
      "+-----------+\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "|       null|\n",
      "+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding a number and a string\n",
    "\n",
    "df.select(df.group + df.n).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "- Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "\n",
    "- Turn your dataframe into a table that can be queried with spark SQL. Name the table my_df. Answer the rest of the questions in this section with a spark sql query (spark.sql) against my_df. After each step, view the first 7 records from the dataframe.\n",
    "\n",
    "\n",
    "- Write a query that shows all of the columns from your dataframe.\n",
    "\n",
    "\n",
    "- Write a query that shows just the n and abool columns from the dataframe.\n",
    "\n",
    "\n",
    "- Write a query that shows just the n and group columns. Rename the group column to g.\n",
    "\n",
    "\n",
    "- Write a query that selects n, and creates two new columns: n2, the original n values halved, and n3: the original n values minus 1.\n",
    "\n",
    "\n",
    "- What happens if you make a SQL syntax error in your query?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('my_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT *\n",
    "FROM my_df\n",
    "    \"\"\"\n",
    ").show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                   n|abool|\n",
      "+--------------------+-----+\n",
      "|  -0.712390662050588|false|\n",
      "|   0.753766378659703|false|\n",
      "|-0.04450307833805...|false|\n",
      "| 0.45181233874578974|false|\n",
      "|  1.3451017084510097|false|\n",
      "|  0.5323378882945463|false|\n",
      "|  1.3501878997225267|false|\n",
      "+--------------------+-----+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT n, abool\n",
    "FROM my_df\n",
    "\"\"\").show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                   n|  g|\n",
      "+--------------------+---+\n",
      "|  -0.712390662050588|  z|\n",
      "|   0.753766378659703|  x|\n",
      "|-0.04450307833805...|  z|\n",
      "| 0.45181233874578974|  y|\n",
      "|  1.3451017084510097|  z|\n",
      "|  0.5323378882945463|  y|\n",
      "|  1.3501878997225267|  z|\n",
      "+--------------------+---+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT n, group AS g\n",
    "FROM my_df\n",
    "\"\"\").show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                   n|                  n2|                  n3|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  -0.712390662050588|  -0.356195331025294|  -1.712390662050588|\n",
      "|   0.753766378659703|  0.3768831893298515|-0.24623362134029703|\n",
      "|-0.04450307833805...|-0.02225153916902...| -1.0445030783380536|\n",
      "| 0.45181233874578974| 0.22590616937289487| -0.5481876612542103|\n",
      "|  1.3451017084510097|  0.6725508542255049| 0.34510170845100974|\n",
      "|  0.5323378882945463| 0.26616894414727316| -0.4676621117054537|\n",
      "|  1.3501878997225267|  0.6750939498612634| 0.35018789972252673|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write a query that selects n, and creates two new columns: \n",
    "# n2,the original n values halved, and n3: the original n values minus 1.\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "SELECT n, (n / 2) AS n2, (n - 1) AS n3\n",
    "FROM my_df\n",
    "\"\"\").show(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type casting\n",
    "\n",
    "- Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "\n",
    "- Use .printSchema to view the datatypes in your dataframe.\n",
    "\n",
    "\n",
    "- Use .dtypes to view the datatypes in your dataframe.\n",
    "\n",
    "\n",
    "- What is the difference between the two code samples below?\n",
    "\n",
    "        df.abool.cast('int')\n",
    "        df.select(df.abool.cast('int')).show()\n",
    "\n",
    "\n",
    "- Use .select and .cast to convert the abool column to an integer type. View the results.\n",
    "\n",
    "\n",
    "- Convert the group column to a integer data type and view the results. What happens?\n",
    "\n",
    "\n",
    "- Convert the n column to a integer data type and view the results. What happens?\n",
    "\n",
    "\n",
    "- Convert the abool column to a string data type and view the results. What happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- n: double (nullable = true)\n",
      " |-- group: string (nullable = true)\n",
      " |-- abool: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n', 'double'), ('group', 'string'), ('abool', 'boolean')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'CAST(abool AS INT)'>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.abool.cast('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|abool|\n",
      "+-----+\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abool', 'int')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(df.abool.cast('int')).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|group|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.group.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                   n|  n|\n",
      "+--------------------+---+\n",
      "|  -0.712390662050588|  0|\n",
      "|   0.753766378659703|  0|\n",
      "|-0.04450307833805...|  0|\n",
      "| 0.45181233874578974|  0|\n",
      "|  1.3451017084510097|  1|\n",
      "+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# casting to an int truncates the number\n",
    "\n",
    "df.select(df.n, df.n.cast('int')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abool: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# casting the abool column to a string instead of a bool\n",
    "\n",
    "df.select(df.abool.cast('string')).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Built-in Functions\n",
    "\n",
    "- Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "\n",
    "- Import the necessary functions from pyspark.sql.functions\n",
    "\n",
    "\n",
    "- Find the highest n value.\n",
    "\n",
    "\n",
    "- Find the lowest n value.\n",
    "\n",
    "\n",
    "- Find the average n value.\n",
    "\n",
    "\n",
    "- Use concat to change the group column to say, e.g. \"Group: x\" or \"Group: y\"\n",
    "\n",
    "\n",
    "- Use concat to combine the n and group columns to produce results that look like this: \"x: -1.432\" or \"z: 2.352\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            max(n)|\n",
      "+------------------+\n",
      "|2.1503829673811126|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            min(n)|\n",
      "+------------------+\n",
      "|-1.261605945319069|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|            avg(n)|\n",
      "+------------------+\n",
      "|0.3664026449885217|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(avg(df.n)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|concat(Group: , group)|\n",
      "+----------------------+\n",
      "|              Group: z|\n",
      "|              Group: x|\n",
      "|              Group: z|\n",
      "|              Group: y|\n",
      "|              Group: z|\n",
      "|              Group: y|\n",
      "|              Group: z|\n",
      "|              Group: x|\n",
      "|              Group: z|\n",
      "|              Group: y|\n",
      "|              Group: x|\n",
      "|              Group: y|\n",
      "|              Group: y|\n",
      "|              Group: y|\n",
      "|              Group: y|\n",
      "|              Group: x|\n",
      "|              Group: z|\n",
      "|              Group: y|\n",
      "|              Group: x|\n",
      "|              Group: x|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df.show(2)\n",
    "df.select(concat(lit('Group: '), df.group)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|concat(group, : , round(n, 3))|\n",
      "+------------------------------+\n",
      "|                     z: -0.712|\n",
      "|                      x: 0.754|\n",
      "|                     z: -0.045|\n",
      "|                      y: 0.452|\n",
      "|                      z: 1.345|\n",
      "|                      y: 0.532|\n",
      "|                       z: 1.35|\n",
      "|                      x: 0.861|\n",
      "|                      z: 1.479|\n",
      "|                     y: -1.045|\n",
      "|                     x: -0.789|\n",
      "|                     y: -1.262|\n",
      "|                      y: 0.563|\n",
      "|                     y: -0.243|\n",
      "|                      y: 0.914|\n",
      "|                      x: 0.317|\n",
      "|                      z: 0.127|\n",
      "|                       y: 2.15|\n",
      "|                      x: 0.606|\n",
      "|                     x: -0.027|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df.select(concat(df.group, lit(': '), round(df.n, 3))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------------------+\n",
      "|concat(Group: , group)|concat(group, : , round(n, 3))|\n",
      "+----------------------+------------------------------+\n",
      "|              Group: z|                     z: -0.712|\n",
      "|              Group: x|                      x: 0.754|\n",
      "|              Group: z|                     z: -0.045|\n",
      "|              Group: y|                      y: 0.452|\n",
      "|              Group: z|                      z: 1.345|\n",
      "+----------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    concat(lit('Group: '), df.group),\n",
    "    concat(df.group, lit(': '), round(df.n, 3))\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter / Where\n",
    "\n",
    "- Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "\n",
    "- Use .filter or .where to select just the rows where the group is y and view the results.\n",
    "\n",
    "\n",
    "- Select just the columns where the abool column is false and view the results.\n",
    "\n",
    "\n",
    "- Find the columns where the group column is not y.\n",
    "\n",
    "\n",
    "- Find the columns where n is positive.\n",
    "\n",
    "\n",
    "- Find the columns where abool is true and the group column is z.\n",
    "\n",
    "\n",
    "- Find the columns where abool is true or the group column is z.\n",
    "\n",
    "\n",
    "- Find the columns where abool is false and n is less than 1\n",
    "\n",
    "\n",
    "- Find the columns where abool is false or n is less than 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|0.45181233874578974|    y|false|\n",
      "| 0.5323378882945463|    y|false|\n",
      "|-1.0453771305385342|    y| true|\n",
      "| -1.261605945319069|    y|false|\n",
      "| 0.5628467852810314|    y| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter using filter for rows where the group is y\n",
    "\n",
    "df.filter(df.group == 'y').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|0.45181233874578974|    y|false|\n",
      "| 0.5323378882945463|    y|false|\n",
      "|-1.0453771305385342|    y| true|\n",
      "| -1.261605945319069|    y|false|\n",
      "| 0.5628467852810314|    y| true|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter using where for rows where the group is y\n",
    "\n",
    "df.where(df.group == 'y').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for rows where abool is False\n",
    "\n",
    "df.filter(df.abool == False).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for rows where the group is not y\n",
    "\n",
    "df.filter(df.group != 'y').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+-----+\n",
      "|                  n|group|abool|\n",
      "+-------------------+-----+-----+\n",
      "|  0.753766378659703|    x|false|\n",
      "|0.45181233874578974|    y|false|\n",
      "| 1.3451017084510097|    z|false|\n",
      "| 0.5323378882945463|    y|false|\n",
      "| 1.3501878997225267|    z|false|\n",
      "+-------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter for rows where n is positive\n",
    "\n",
    "df.filter(df.n > 0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----+\n",
      "|                 n|group|abool|\n",
      "+------------------+-----+-----+\n",
      "|1.4786857374358966|    z| true|\n",
      "+------------------+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the columns where abool is true and the group column is z.\n",
    "\n",
    "df.filter(df.group == 'z').where(df.abool == True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "|  1.4786857374358966|    z| true|\n",
      "| -1.0453771305385342|    y| true|\n",
      "|  0.5628467852810314|    y| true|\n",
      "|-0.24332625188556253|    y| true|\n",
      "| 0.12730328020698067|    z|false|\n",
      "|  2.1503829673811126|    y| true|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the columns where abool is true OR the group column is z.\n",
    "\n",
    "df.filter((df.group == 'z') | (df['abool'] == True)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "| -0.7889890249515489|    x|false|\n",
      "|  -1.261605945319069|    y|false|\n",
      "|  0.9137407048596775|    y|false|\n",
      "| 0.31735092273633597|    x|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the columns where abool is false and n is less than 1\n",
    "\n",
    "df.filter((df.abool == False) & (df.n < 1)).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-----+\n",
      "|                   n|group|abool|\n",
      "+--------------------+-----+-----+\n",
      "|  -0.712390662050588|    z|false|\n",
      "|   0.753766378659703|    x|false|\n",
      "|-0.04450307833805...|    z|false|\n",
      "| 0.45181233874578974|    y|false|\n",
      "|  1.3451017084510097|    z|false|\n",
      "|  0.5323378882945463|    y|false|\n",
      "|  1.3501878997225267|    z|false|\n",
      "|  0.8612113741693206|    x|false|\n",
      "| -1.0453771305385342|    y| true|\n",
      "| -0.7889890249515489|    x|false|\n",
      "+--------------------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the columns where abool is false or n is less than 1\n",
    "\n",
    "df.filter((df.abool == False) | (df.n < 1)).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When / Otherwise\n",
    "\n",
    "- Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "\n",
    "- Use when and .otherwise to create a column that contains the text \"It is true\" when abool is true and \"It is false\"\" when abool is false.\n",
    "\n",
    "\n",
    "- Create a column that contains 0 if n is less than 0, otherwise, the original n value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n",
      "|abool|True or False?|\n",
      "+-----+--------------+\n",
      "|false|   It is false|\n",
      "|false|   It is false|\n",
      "|false|   It is false|\n",
      "|false|   It is false|\n",
      "|false|   It is false|\n",
      "|false|   It is false|\n",
      "|false|   It is false|\n",
      "|false|   It is false|\n",
      "| true|    It is true|\n",
      "| true|    It is true|\n",
      "+-----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use when and .otherwise to create a column that contains the text \n",
    "#\"It is true\" when abool is True and \"It is false\"\" when abool is False.\n",
    "\n",
    "df.select(\n",
    "    df.abool, when(df.abool == True, 'It is true')\n",
    "    .otherwise('It is false').alias('True or False?')\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CASE WHEN (n > 0) THEN n ELSE 0 END|\n",
      "+-----------------------------------+\n",
      "|                                0.0|\n",
      "|                  0.753766378659703|\n",
      "|                                0.0|\n",
      "|                0.45181233874578974|\n",
      "|                 1.3451017084510097|\n",
      "|                 0.5323378882945463|\n",
      "|                 1.3501878997225267|\n",
      "|                 0.8612113741693206|\n",
      "|                 1.4786857374358966|\n",
      "|                                0.0|\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column that contains 0 if n is less than 0, \n",
    "# otherwise, the original n value.\n",
    "\n",
    "df.select(\n",
    "    when(df.n > 0, df.n)\n",
    "    .otherwise(0)\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting\n",
    "\n",
    "- Use the starter code above to re-create a spark dataframe.\n",
    "\n",
    "\n",
    "- Sort by the n value.\n",
    "\n",
    "\n",
    "- Sort by the group value, both ascending and descending.\n",
    "\n",
    "\n",
    "- Sort by the group value first, then, within each group, sort by n value.\n",
    "\n",
    "\n",
    "- Sort by abool, group, and n. Does it matter in what order you specify the columns when sorting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
