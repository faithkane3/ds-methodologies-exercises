{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "    -Logistic regression estimates the probability of a certain event occurring.\n",
    "    \n",
    "    -Unlike ordinary regression, LR does not assume that the relationship between the independent and dependent variables are linear.\n",
    "    \n",
    "    -LR is a variation of ordinary regression that is used when the dependent (response) variable is dichotomous.\n",
    "    \n",
    "    -LR forms a predictor variables that is a linear combination of the explanatory variables.\n",
    "    \n",
    "    -The values of this predictor variable are then transformed into probabilities by a logistic function which has the shape of an S.\n",
    "    \n",
    "    -The values of the predictor variable are displayed on the horizontal axis, and the probabilities are on the vertical axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydataset import data\n",
    "# data()\n",
    "# import seaborn sns\n",
    "# sns.get_dataset_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Define variable and target dataframes as x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(columns=[\"species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[[\"species\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Split data into x and y train and x and y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .30, random_state = 123) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Create the logistic regression object\n",
    "\n",
    "-Choose the best solver for the dataset Iris\n",
    "\n",
    "    -For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and ‘saga’ are faster for large ones. \n",
    "    \n",
    "    -‘liblinear’ and ‘saga’ also handle L1 penalty\n",
    "    \n",
    "    \n",
    "    -‘liblinear’ does not handle no penalty\n",
    "                                                \n",
    "                                                (scikit-learn.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state = 123, solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Fit the logistic regression object to the train data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=123, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Pring coefficients and intercept of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient: \n",
      " [[ 0.38421538  1.32718255 -2.11307588 -0.94269552]\n",
      " [ 0.43099717 -1.34596217  0.4506587  -1.07117492]\n",
      " [-1.517952   -1.52141607  2.26046444  2.12613123]]\n",
      "Intercept: \n",
      " [ 0.25726194  0.58107381 -0.87235291]\n"
     ]
    }
   ],
   "source": [
    "print('Coefficient: \\n', logit.coef_)\n",
    "print('Intercept: \\n', logit.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Estimate whether you can predict the species using the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logit.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Estimate the probability that you can predict the species using the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = logit.predict_proba(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Compute the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on training set: 0.95\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on training set: {:.2f}'\n",
    "     .format(logit.score(x_train, y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Create a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32  0  0]\n",
      " [ 0 36  4]\n",
      " [ 0  1 32]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Compute Precision, Recall, F1-score, and Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        32\n",
      "  versicolor       0.97      0.90      0.94        40\n",
      "   virginica       0.89      0.97      0.93        33\n",
      "\n",
      "    accuracy                           0.95       105\n",
      "   macro avg       0.95      0.96      0.95       105\n",
      "weighted avg       0.95      0.95      0.95       105\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Logistic Regression classifier on test set: 0.98\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Logistic Regression classifier on test set: {:.2f}'\n",
    "     .format(logit.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Visualize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a17a7f438>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred_proba = [i[1] for i in y_pred_proba]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(y_pred_proba, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-After creating the LR object with both solvers, Saga and Liblinear, I saw no difference in the 98% accuracy of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Exercises\n",
    "\n",
    "-A sequence of rules that can be used to classify 2 or more classes\n",
    "\n",
    "-Each node represents a single input variable (x) and a split point or class of that variable\n",
    "\n",
    "-The leaf nodes of the tree contain an output variable (y) which is used to make a prediction.\n",
    "\n",
    "-Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n",
    "\n",
    "##### Pros\n",
    "\n",
    "-Simple to understand\n",
    " \n",
    "-Simple to visualize\n",
    "\n",
    "-Simple to explain the output\n",
    "\n",
    "-Requires little data preparation\n",
    "\n",
    "-Can handle both numerical and categorical data\n",
    "\n",
    "-Perform well for a broad range of problems\n",
    "\n",
    "##### Cons\n",
    "\n",
    "-Can create complex trees that do not generalise well.\n",
    "\n",
    "-Can be unstable because small variations in the data might lead to overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example\n",
    "\n",
    "-Iris dataset: Identifing the iris species based on 4 measurements\n",
    "\n",
    "-We could come up with a probability of each feature adding independently to the overall poisonousness of a mushroom. This would be a Naive Bayesian Classification but has the problem that each feature isn’t independent. There is crossover. Maybe a mushroom that is bright and flat is okay to eat while a mushroom that is bright and round isn’t.\n",
    "\n",
    "-Instead, to build this decision tree we can take the overall algorithm:\n",
    "\n",
    "    1. Split data into subcategories using the most informational attribute.\n",
    "\n",
    "    2. Keep going until threshold.\n",
    "    \n",
    "-This algorithm is quite simple. The idea is to take the entire population of mushrooms, and split them into subcategories until we have a tree showing us how to classify something."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vocab\n",
    "\n",
    "##### Entropy and Information Gain\n",
    "\n",
    "##### ENTROPY\n",
    "\n",
    "-A way of determining just how descriptive bits are. A canonical example of entropy would be that if it’s always sunny in Death Valley with a probability of 100% then the entropy would be 0 to send information about what the weather of the day was. The information doesn’t need to be encoded since there’s nothing to report. Another example of high entropy would be having a complex password. The more numerous and diverse the characters you use, the higher the entropy. The same is true of attributes. If we have lots of possibilities for mushroom odor, then that would have higher entropy.\n",
    "\n",
    "##### INFORMATION GAIN\n",
    "\n",
    "Entropy is used to calculate information gain. Information gain is the main key that is used by Decision Tree Algorithms to construct a Decision Tree. Decision Tree algorithm will always try to maximize information gain. An attribute with highest information gain will tested/split first.\n",
    "Advantage of using information gain as a hyper-parameter: \"Information gain ratio biases the decision tree against considering attributes with a large number of distinct values. So it solves the drawback of information gain—namely, information gain applied to attributes that can take on a large number of distinct values might learn the training set too well. For example, suppose that we are building a decision tree for some data describing a business's customers. Information gain is often used to decide which of the attributes are the most relevant, so they can be tested near the root of the tree. One of the input attributes might be the customer's credit card number. This attribute has a high information gain, because it uniquely identifies each customer, but we do not want to include it in the decision tree: deciding how to treat a customer based on their credit card number is unlikely to generalize to customers we haven't seen before.\"\n",
    "\n",
    "##### Gini Impurity\n",
    "\n",
    "-A probabilistic measure - how probable an attribute is at showing up and the probability of it being mistaken. If the GINI impurity for attribute 1 is lower than the GINI impurity for attribute 2, while creating a decision tree we would want to choose attribute 1 as a split point first, since it will create less impurity in the tree. (Not to be confused with the GINI coefficient.)\n",
    "\n",
    "##### Pruning the trees\n",
    "\n",
    "-Goal is to find a subtree of the full decision tree using the preceding decision points that minimizes this error surface\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fit the decision tree classifier to your training sample and transform (i.e. make predictions on the training sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal.Length</th>\n",
       "      <th>Sepal.Width</th>\n",
       "      <th>Petal.Length</th>\n",
       "      <th>Petal.Width</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sepal.Length  Sepal.Width  Petal.Length  Petal.Width Species\n",
       "1           5.1          3.5           1.4          0.2  setosa\n",
       "2           4.9          3.0           1.4          0.2  setosa\n",
       "3           4.7          3.2           1.3          0.2  setosa\n",
       "4           4.6          3.1           1.5          0.2  setosa\n",
       "5           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydataset import data\n",
    "\n",
    "\n",
    "df = data(\"iris\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.lower().replace('.', '_') for col in df]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "1           5.1          3.5           1.4          0.2  setosa\n",
       "2           4.9          3.0           1.4          0.2  setosa\n",
       "3           4.7          3.2           1.3          0.2  setosa\n",
       "4           4.6          3.1           1.5          0.2  setosa\n",
       "5           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Split the dataframe into train and test, 70/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width\n",
       "115           5.8          2.8           5.1          2.4\n",
       "137           6.3          3.4           5.6          2.4\n",
       "54            5.5          2.3           4.0          1.3\n",
       "20            5.1          3.8           1.5          0.3\n",
       "39            4.4          3.0           1.3          0.2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.drop(['species'],axis=1)\n",
    "y = df[['species']]\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = .30, random_state = 123)\n",
    "\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification you can change the algorithm to gini or entropy (information gain).  Default is gini.\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model to the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=3,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=123, splitter='best')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['virginica', 'virginica', 'versicolor', 'setosa', 'setosa'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(x_train)\n",
    "y_pred[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estimate the probability of a species\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.5  , 0.5  ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.   , 1.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [1.   , 0.   , 0.   ],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.975, 0.025],\n",
       "       [0.   , 0.5  , 0.5  ],\n",
       "       [0.   , 0.   , 1.   ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = clf.predict_proba(x_train)\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Evaluate your in-sample results using the model score, confusion matrix, and classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree classifier on training set: 0.98\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of Decision Tree classifier on training set: {:.2f}'\n",
    "     .format(clf.score(x_train, y_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32,  0,  0],\n",
       "       [ 0, 40,  0],\n",
       "       [ 0,  2, 31]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(y_train.species.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "versicolor    40\n",
       "virginica     33\n",
       "setosa        32\n",
       "Name: species, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.species.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a classification report\n",
    "\n",
    "-Precision: the higher this number is, the more you were able to pinpoint all positives correctly. If this is a low score, you predicted a lot of positives where there were none. tp / (tp + fp)\n",
    "\n",
    "-Recall: if this score is high, you didn’t miss a lot of positives. But as it gets lower, you are not predicting the positives that are actually there. tp / (tp + fn)\n",
    "\n",
    "-f1-score: The balanced harmonic mean of Recall and Precision, giving both metrics equal weight. The higher the F-Measure is, the better.\n",
    "\n",
    "-Support: number of occurrences of each class in where y is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_dict = classification_report(y_train, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa {'precision': 1.0, 'recall': 1.0, 'f1-score': 1.0, 'support': 32}\n",
      "versicolor {'precision': 0.9523809523809523, 'recall': 1.0, 'f1-score': 0.975609756097561, 'support': 40}\n",
      "virginica {'precision': 1.0, 'recall': 0.9393939393939394, 'f1-score': 0.96875, 'support': 33}\n",
      "accuracy 0.9809523809523809\n",
      "macro avg {'precision': 0.9841269841269842, 'recall': 0.9797979797979798, 'f1-score': 0.9814532520325203, 'support': 105}\n",
      "weighted avg {'precision': 0.981859410430839, 'recall': 0.9809523809523809, 'f1-score': 0.9808870499419281, 'support': 105}\n"
     ]
    }
   ],
   "source": [
    "classification_dict\n",
    "\n",
    "for k, v in classification_dict.items():\n",
    "    print(k, v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print and clearly label the following: Accuracy, true positive rate, false positive rate, true negative rate, false negative rate, precision, recall, f1-score, and support.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(y_train, y_pred))\n",
    "\n",
    "FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "FN = cm.sum(axis=1) - np.diag(cm)\n",
    "TP = np.diag(cm)\n",
    "TN = cm.values.sum() - (FP + FN + TP)\n",
    "\n",
    "# Sensitivity, hit rate, recall, or true positive rate\n",
    "TPR = TP/(TP+FN)\n",
    "# Specificity or true negative rate\n",
    "TNR = TN/(TN+FP) \n",
    "# Precision or positive predictive value\n",
    "PPV = TP/(TP+FP)\n",
    "# Negative predictive value\n",
    "NPV = TN/(TN+FN)\n",
    "# Fall out or false positive rate\n",
    "FPR = FP/(FP+TN)\n",
    "# False negative rate\n",
    "FNR = FN/(TP+FN)\n",
    "# False discovery rate\n",
    "FDR = FP/(TP+FP)\n",
    "\n",
    "# Overall accuracy\n",
    "ACC = (TP+TN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    2\n",
       "2    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Run through steps 2-4 using entropy as your measure of impurity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Which performs better on your in-sample data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "##### Algorithm Overview\n",
    "\n",
    "-Random forest is a type of ensemble ML algorithm called Bootstrap Aggregation or bagging.\n",
    "\n",
    "-Bootstrapping is a statistical method for estimating a quantity from a data sample, e.g. mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value. In bagging, the same approach is used for estimating entire statistical models, such as decision trees. Multiple samples of your training data are taken and models are constructed for each sample set.\n",
    "\n",
    "-When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.\n",
    "\n",
    "-Random forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness. The models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.\n",
    "\n",
    "-If you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
