{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "\n",
    "-Predicting categorical outcomes\n",
    "\n",
    "-Supervised machine learning techinque\n",
    "\n",
    "-Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary:\n",
    "\n",
    "-Classifier: An algorithm that maps the input data to a specific category.\n",
    "\n",
    "-Classification model: series of steps that take the patterns of input variables, generalize those patterns, and apply to new data in order to predict the class.\n",
    "\n",
    "-Feature: A feature, aka input/independent variable, is an individual measurable property of a phenomenon being observed.\n",
    "\n",
    "-Binary Classification: Classification with two possible outcomes. e.g. pass/fail\n",
    "\n",
    "-Multi class classification: Classification with more than two classes, where each sample is assigned to one and only one target label. e.g. Students' grade level in school (1st-12th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Classification Algorithms\n",
    "\n",
    "-Logistic Regression (sklearn.linear_model.LogisticRegression)\n",
    "\n",
    "-Decision Tree (sklearn.tree.DecisionTreeClassifier)\n",
    "\n",
    "-Naive Bayes (sklearn.naive_bayes.BernoulliNB)\n",
    "\n",
    "-K-Nearest Neighbors (sklearn.neighbors.KNeighborsClassifier)\n",
    "\n",
    "-Random Forest (sklearn.ensemble.RandomForestClassifier)\n",
    "\n",
    "-Support Vector Machine (sklearn.svm.SVC)\n",
    "\n",
    "-Stochastic Gradient Descent (sklearn.linear_model.SGDClassifier)\n",
    "\n",
    "-AdaBoost (sklearn.ensemble.AdaBoostClassifier)\n",
    "\n",
    "-Bagging (sklearn.ensemble.BaggingClassifier)\n",
    "\n",
    "-Gradient Boosting (sklearn.ensemble.GradientBoostingClassifier)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression      sklearn.linear_model.LogisticRegression\n",
    "\n",
    "-Technically a regression algorithm (Goal is to find the values for the coefficients that weight each input variable).\n",
    "\n",
    "-Used to predicting binary outcomes.\n",
    "\n",
    "-The output is a value between 0 and 1 that represents the probability of one class over the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Curse of Dimensionality\n",
    "\n",
    "-When you add more features, the size of your sample becomes critical.\n",
    "\n",
    "-Know how this impacts different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree (CART: Classification and Regression Trees)\n",
    "\n",
    "-A sequence of rules used to classify 2 or more classes\n",
    "\n",
    "-Each node represents a single input variable (x) and a split point or class of that variable\n",
    "\n",
    "    -The split points are your parameters\n",
    "    \n",
    "-The leaf nodes of the tree contain an output variable (y) which is used to make a prediction. (End Points)\n",
    "\n",
    "-Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n",
    "\n",
    "-Returns split points and stores the split points in the model.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    -Can handle both numerical and categorical data.\n",
    "\n",
    "    -Performs well for a braod range of problems.\n",
    "\n",
    "    -Requires little data preparation.\n",
    "    \n",
    "Cons:\n",
    "\n",
    "    -Can be unstable because small variations in the data might lead to overfitting\n",
    "    \n",
    "    -Risk of overfitting: can create complex trees that do not gereralise well. You can get it down to 100% accuracy, and that would be too specific, overfit. You have to know when to cut it off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "-Random forest is an implementation of bootstrap aggregation, aka bagging, which is an ensemble algorithm.\n",
    "\n",
    "-Bootstrapping is a statistical method for estimating a quantity from a data sample, e.g. mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value. In bootstrap aggregation, or bagging, the same approach is used for estimating entire statistical models, such as decision trees. Multiple samples of your training data are taken and models are constructed for each sample set.\n",
    "\n",
    "-When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.\n",
    "\n",
    "-Random forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness. The models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.\n",
    "\n",
    "-If you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    -Less risk of over_fitting than decision tree.\n",
    "    \n",
    "    -More accurate than decision trees in most cases.\n",
    "    \n",
    "Cons:\n",
    "\n",
    "    -High demand on computational resources\n",
    "    \n",
    "    -Difficult to implement.\n",
    "    \n",
    "    -somewhat of a blackbox model, difficult to explain. You don't know what the trees are. If 70 trees return red and 30 green, the answer is red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbor    (This is NOT K-Means)\n",
    "\n",
    "##### Description\n",
    "\n",
    "-KNN makes predictions based on how close a new data point is to known data points.\n",
    "\n",
    "-It is considered \"lazy\" as it does not attempt to construct a general internal model, but simply stores instances of the training data. Classification is computed from a simple majority vote of the k nearest neighbours of each point.\n",
    "\n",
    "-Predictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression problems, this might be the mean output variable. For classification problems this might be the mode (or most common) class value.\n",
    "\n",
    "-It is important to define a metric to measure how similar data instances are. Euclidean distance can be used if attributes are all on the same scale (or you convert them to the same scale).\n",
    "\n",
    "Pros:\n",
    "\n",
    "    -Simple to implement\n",
    "    \n",
    "    -Robust to noise\n",
    "    \n",
    "    -Performs calculations \"just in time\", i.e. when a prediction is needed (as opposed to ahead of time)\n",
    "    \n",
    "    -Training instances can be updated and curated over time to keep predictions accurate.\n",
    "\n",
    "Cons:\n",
    "\n",
    "    -Need to determine the value of K\n",
    "    \n",
    "    -High computational cost:  it has to compute the distance of each instance to all the training samples... you have to hang onto your entire training set.\n",
    "    \n",
    "    -Not great with huge amounts of data bc it stores entire training set\n",
    "    \n",
    "    -Curse of dimensionality: Distance can break down in very high dimensions, negatively affecting the performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine\n",
    "\n",
    "-A technique that uses higher dimensions to best seperate data points into two classes.\n",
    "\n",
    "-Support Vector Machines select hyperplane (a line that splits the input variable space) to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions, you can visualize this as a line.\n",
    "\n",
    "-An optimization algorithm is used to find the values for the coefficients that maximizes the margin. The distance between the hyperplane and the closest data points is referred to as the margin. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points, called the support vectors, are relevant in defining the hyperplane and in the construction of the classifier.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    -Effective in high dimensional spaces.\n",
    "    \n",
    "    -Memory efficient: Uses a subset of training points in the decision function.\n",
    "    \n",
    "    -Highly successful classifier\n",
    "    \n",
    "Cons:\n",
    "\n",
    "    -Does not directly provide probabilty estimates, these are calculated using an expensive five-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes\n",
    "\n",
    "-Naive Bayes is based on Bayesâ€™ theorem that assumes independence between every pair of features.\n",
    "\n",
    "-It is comprised of two types of probabilities that can be calculated directly from your training data:\n",
    "\n",
    "    -The probability of each class\n",
    "    \n",
    "    -The conditional probability for each class given each x value\n",
    "\n",
    "-Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem. When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities. (so normalize your data!)\n",
    "\n",
    "-It assumes that each input variable is independent (which is often not the case), thus it is called \"naive\". This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems, including document classification and spam filtering.\n",
    "\n",
    "Pros:\n",
    "\n",
    "    -Works with a smaller sample size of training data than other classifiers.\n",
    "    \n",
    "    -Extremely fast compated to more sophisticated methods.\n",
    "    \n",
    "    -Simple and powerful\n",
    "    \n",
    "Cons:\n",
    "\n",
    "    -Can be a bad estimator if used in less than ideal problems.\n",
    "    \n",
    "Use Cases:\n",
    "\n",
    "-Based on their purchase and browsing history, what promos should I offer to my customers?\n",
    "\n",
    "-Learn from IB to develop methods for prospecting new customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process for each of the models above:\n",
    "\n",
    "-Create the object\n",
    "\n",
    "-Fit\n",
    "\n",
    "-Predict \n",
    "\n",
    "    -New observations or observations that don't have lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
