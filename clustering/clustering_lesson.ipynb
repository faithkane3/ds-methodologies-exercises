{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "- No labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Clustering is an unsupervised machine learning methodology for grouping and identifing similar objects, people, or observations.\n",
    "\n",
    "    \n",
    "    - We can create a new feature (or predictor) from this using these cluster ids, and use it in your ML or as a target.\n",
    "\n",
    "\n",
    "- Clustering is often used as a preprocessing or an exploratory step in the data science pipeline so that the cluster that each item is assigned to becomes a feature for a supervised model.\n",
    "\n",
    "\n",
    "- In this module, you will be introduced to various clustering algorithms and learn why and when to use them. You will learn how to use clustering methods to identify similar groups using Python using Scikit-Learn. You will learn how apply these clusters further down the pipeline.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cases\n",
    "\n",
    "- Text: Document classification, summarization, topic modeling, recommendations\n",
    "\n",
    "\n",
    "- Geographic: crime zones, housing prices\n",
    "\n",
    "\n",
    "- Marketing: Customer segmentation, market research\n",
    "\n",
    "\n",
    "- Anomaly detection: account takeover, security risk, fraud\n",
    "\n",
    "\n",
    "- Image processing: radiology, security\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "\n",
    "- Euclidean Distance\n",
    "\n",
    "\n",
    "- Manhattan Distance\n",
    "\n",
    "\n",
    "- Cosine Similarity\n",
    "\n",
    "\n",
    "- Sparse vs. Dense Matrix\n",
    "\n",
    "\n",
    "- Manhattan (Taxicab) vs Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types\n",
    "\n",
    "- Input: continuous data, or ordered discrete data at a minimum.\n",
    "\n",
    "\n",
    "- Output: Integer representing a cluster id.\n",
    "\n",
    "\n",
    "    - The number itself doesn't mean anything except that those who share the same number are most similar. In addition, the number doesn't compare to any of the other cluster id's beyond the fact that they are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Clustering Agorithms\n",
    "\n",
    "### K-Means\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Description\n",
    "\n",
    "\n",
    "    - most popular \"clustering\" algorithms.\n",
    "\n",
    "\n",
    "    - stores k centroids that it uses to define clusters.\n",
    "\n",
    "\n",
    "    - A point is considered to be in a particular cluster if it is closer to that cluster's centroid than any other centroid.\n",
    "    \n",
    "    \n",
    "    - K-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters.\n",
    "    \n",
    "    \n",
    "    - Python implementation: sklearn.cluster.KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PARAMETERS\n",
    "\n",
    "\n",
    "- Number of clusters (k): The number of clusters to form, which is equal to the number of centroids to generate\n",
    "\n",
    "\n",
    "- Number of initializations (n_init): The number of times the algorithm will 'begin', i.e. kick off with different centroid seeds\n",
    "\n",
    "\n",
    "- Maximum Number of iterations (max_iter): If the algorithm doesn't converge prior, this is the maximum number of times the algorithm will loop through re-calculation of the centroids.\n",
    "\n",
    "\n",
    "- random_state: Specific to sklearn, this is for 'setting the seed' for reproducibility. When you use any integer as a value here and then re-run with the same value, the algorithm will kick off with the same seed as before, thus the same observations & centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pros\n",
    "\n",
    "\n",
    "1. Performance scales well with the amount of data, i.e. the algorithm is linear in the number of objects $O(n)$\n",
    "\n",
    "2. Creates tighter, more refined clusters\n",
    "\n",
    "3. Centroids can be recomputed driving an observation or object to another cluster\n",
    "\n",
    "\n",
    "- Cons\n",
    "\n",
    "\n",
    "1. naive use of the mean value for the cluster center\n",
    "\n",
    "\n",
    "2. fails when the clusters are not circular\n",
    "\n",
    "\n",
    "3. Hard to predict what k (the number of clusters) should be\n",
    "\n",
    "\n",
    "4. Which observations the clustering starts with, i.e. initial seeds, can dramatically affect the results\n",
    "\n",
    "\n",
    "5. The order of the data can affect the results\n",
    "\n",
    "\n",
    "6. Results are extremely sensitive to the scale of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes from DBScan lesson\n",
    "\n",
    "- Clusters is about Density and Distance\n",
    "\n",
    "- DBScan decides how many clusters, gives us labels, decides on outliers, uses conditions to decide on clusters\n",
    "\n",
    "- Hyperparameters:\n",
    "\n",
    "    - distance function (ie. - Euclidean)\n",
    "\n",
    "    - eps: distance (epsilon), radius for the center of a cluster\n",
    "\n",
    "    - minPts: # of points required to define a cluster\n",
    "\n",
    "        - core point has minPts within its eps-neighborhood\n",
    "\n",
    "        - reachable point is in the eps-neighborhood of a core point\n",
    "\n",
    "        - if neither of the above conditions are met, the point is an outlier\n",
    "    \n",
    "- Drawbacks:\n",
    "\n",
    "    - The only interpretation of this is distance and space\n",
    "\n",
    "- Advantages:\n",
    "    \n",
    "    - Better with smaller datasets, computationally expensive\n",
    "\n",
    "    - Outlier Detection (-1)\n",
    "    \n",
    "    - Finding anomalies in more than 2D\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/faith/codeup-data-science/ds-methodologies-exercises/clustering\n"
     ]
    }
   ],
   "source": [
    "# this is a cool way if you need to\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on End-to-End Clustering Example\n",
    "\n",
    "- You can use clustering to create features to predict a target variable\n",
    "\n",
    "    - OHE cluster IDs\n",
    "    \n",
    "    - Cluster on 30 variables\n",
    "    \n",
    "    - 8 Clusters remain\n",
    "    \n",
    "    - Find out which clusters add value to your prediction model.\n",
    "    \n",
    "        - You want your clusters to be somewhat balanced.\n",
    "        \n",
    "        - Use viz to identify clusters that matter to you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on Zillow Data for Project\n",
    "\n",
    "- Audience is your class\n",
    "\n",
    "- Work with a partner that is assigned\n",
    "\n",
    "- Choose one of three ways to apply clustering\n",
    "\n",
    "- Talk about the highlights or discoveries from the data and from the project itself, from your findings. \n",
    "\n",
    "    - what you learned as it relates to the domain, to the data, and to data science.\n",
    "    \n",
    "- Deliverable: notebook, supporting files, modules separated out for ease of walking the class through your project.\n",
    "\n",
    "- Clustering:\n",
    "\n",
    "    - Use the clusters as possible features, model with them.\n",
    "    \n",
    "    - Use the clusters for exploration, to make discoveries, to decide on a group of variables to dive into or a group of features that may be a waste of time/resources.\n",
    "    \n",
    "    - Use the clusters for your target, an option for binning.\n",
    "    \n",
    "- Analysis/Takeaway and Modeling\n",
    "\n",
    "    - The aim is to predict your target variable (logerror)\n",
    "    \n",
    "    - Your model should be able to guess which zestimates will be the most inaccurate, the patterns in the residuals\n",
    "    \n",
    "    - You have a continuous variable that you want to predict, in a nutshell\n",
    "    \n",
    "- Process:\n",
    "\n",
    "    - Acquire data\n",
    "    \n",
    "    - Prep: Handle Nulls, Outliers, Viz Dist., Drop Variables\n",
    "    \n",
    "    - Split\n",
    "    \n",
    "        - Impute here if you are going to. Fit the imputer using the train data.\n",
    "        \n",
    "            - fit returns the mean, transform imputes the mean onto the train and test\n",
    "    \n",
    "    - Scale: Think about how you want to scale the diff data types\n",
    "    \n",
    "    - Explore: Viz, stats testing, clustering, crosstabs, think about the data types of the variables you are comparing. The dtypes will determine how to viz and what kinds of stats tests to run\n",
    "    \n",
    "        - You have to think about aggregating sometimes when the data is too granular to really visualize.\n",
    "    \n",
    "- Must haves:\n",
    "\n",
    "    -project planning, modules, stats testing of clusters and features, viz of clusters, clusters, model, summary of key drivers of target\n",
    "    \n",
    "        - All planning in README: goals, hypothesis, doodles and graph ideas, data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
